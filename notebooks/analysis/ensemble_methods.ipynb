{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:50.560005Z",
     "start_time": "2025-04-28T21:13:50.539958Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import json\n",
    "import random\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "sns.color_palette(\"Blues\", as_cmap=True)\n",
    "\n",
    "# UNKNOWN_PROJECT = 'ml-julka/2_classes_run_optimal_configs_ViT'\n",
    "# KNOWN_PROJECT = 'ml-julka/11_classes_run_optimal_configs_ViT'\n",
    "UNKNOWN_PROJECT = 'ml-julka/2_classes_run_optimal_configs_M5'\n",
    "KNOWN_PROJECT = 'ml-julka/11_classes_run_optimal_configs_M5'\n",
    "NAME = 'm5_ensemble_of_5'\n",
    "MODE = 'testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442215721fe4a474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:50.575531Z",
     "start_time": "2025-04-28T21:13:50.565512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 runs for unknown.\n"
     ]
    }
   ],
   "source": [
    "log_dir_2 = os.path.join(\"..\", \"..\", \"logs\", UNKNOWN_PROJECT)\n",
    "if not os.path.exists(log_dir_2):\n",
    "    print(f\"Folder '{log_dir_2}' does not exist.\")\n",
    "else:\n",
    "    run_dirs_2 = sorted(glob(os.path.join(log_dir_2, 'run_*')),\n",
    "                      key=lambda x: int(os.path.basename(x).split('_')[1]))\n",
    "    print(f\"Found {len(run_dirs_2)} runs for unknown.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4285a31386921b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:50.747878Z",
     "start_time": "2025-04-28T21:13:50.734024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 runs for known.\n"
     ]
    }
   ],
   "source": [
    "log_dir_11 = os.path.join(\"..\", \"..\", \"logs\", KNOWN_PROJECT)\n",
    "if not os.path.exists(log_dir_11):\n",
    "    print(f\"Folder '{log_dir_11}' does not exist.\")\n",
    "else:\n",
    "    run_dirs_11 = sorted(glob(os.path.join(log_dir_11, 'run_*')),\n",
    "                      key=lambda x: int(os.path.basename(x).split('_')[1]))\n",
    "    print(f\"Found {len(run_dirs_11)} runs for known.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6faff6b2842e954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:51.107187Z",
     "start_time": "2025-04-28T21:13:51.092136Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_results(run_dirs):\n",
    "    max_accuracies = []\n",
    "    for run_dir in run_dirs[1:]:\n",
    "        val_csv = os.path.join(run_dir, 'validation_metrics.csv')\n",
    "        if os.path.exists(val_csv):\n",
    "            df = pd.read_csv(val_csv)\n",
    "            max_acc = df['acc/val'].max()\n",
    "            run_num = os.path.basename(run_dir).split('_')[1]\n",
    "            max_accuracies.append((int(run_num), max_acc))\n",
    "\n",
    "    results = pd.DataFrame(max_accuracies, columns=['Run', 'Max Validation Accuracy'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26513c781ef58fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:51.122600Z",
     "start_time": "2025-04-28T21:13:51.112400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Summary of 11 class:\n",
      "count    4.000000\n",
      "mean     0.894687\n",
      "std      0.002953\n",
      "min      0.891437\n",
      "25%      0.892584\n",
      "50%      0.894878\n",
      "75%      0.896980\n",
      "max      0.897554\n",
      "Name: Max Validation Accuracy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_11 = get_results(run_dirs_11)\n",
    "print(\"\\nStatistical Summary of 11 class:\")\n",
    "print(results_11['Max Validation Accuracy'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936c0af58deef056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:51.325686Z",
     "start_time": "2025-04-28T21:13:51.296073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Summary of 2 class:\n",
      "count    4.000000\n",
      "mean     0.929136\n",
      "std      0.008008\n",
      "min      0.920287\n",
      "25%      0.926101\n",
      "50%      0.928258\n",
      "75%      0.931293\n",
      "max      0.939740\n",
      "Name: Max Validation Accuracy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_2 = get_results(run_dirs_2)\n",
    "print(\"\\nStatistical Summary of 2 class:\")\n",
    "print(results_2['Max Validation Accuracy'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e8686e28da84d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:51.512750Z",
     "start_time": "2025-04-28T21:13:51.482237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Performing Run:\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "best_run_2 = results_2.loc[results_2['Max Validation Accuracy'].idxmax()]\n",
    "\n",
    "print(\"Best Performing Run:\")\n",
    "print(best_run_2['Run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87087da10f2eb47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:51.714518Z",
     "start_time": "2025-04-28T21:13:51.683722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Performing Run:\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "best_run_11 = results_11.loc[results_11['Max Validation Accuracy'].idxmax()]\n",
    "\n",
    "print(\"Best Performing Run:\")\n",
    "print(best_run_11['Run'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "335c5b864e0011d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:51.902949Z",
     "start_time": "2025-04-28T21:13:51.874008Z"
    }
   },
   "outputs": [],
   "source": [
    "UKNOWN_RUN = best_run_2['Run'].astype(int)\n",
    "KNOWN_RUN = best_run_11['Run'].astype(int)\n",
    "\n",
    "LOGS_DIR = '../../logs'\n",
    "\n",
    "CONFIG_UNKNOWN = f'{LOGS_DIR}/{UNKNOWN_PROJECT}/run_{UKNOWN_RUN}/config.json'\n",
    "MODEL_PATH_UNKNOWN = f\"{LOGS_DIR}/{UNKNOWN_PROJECT}/run_{UKNOWN_RUN}/best.pth\"\n",
    "CONFIG_KNOWN = f'{LOGS_DIR}/{KNOWN_PROJECT}/run_{KNOWN_RUN}/config.json'\n",
    "MODEL_PATH_KNOWN = f\"{LOGS_DIR}/{KNOWN_PROJECT}/run_{KNOWN_RUN}/best.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff020da299a50308",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:52.119946Z",
     "start_time": "2025-04-28T21:13:52.090403Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "with open(CONFIG_KNOWN) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, dictionary):\n",
    "        for k, v in dictionary.items():\n",
    "            if isinstance(v, dict):\n",
    "                setattr(self, k, Config(v))\n",
    "            else:\n",
    "                setattr(self, k, v)\n",
    "\n",
    "cfg = Config(cfg)\n",
    "cfg.data.root = os.path.join('..', '..', 'data')\n",
    "\n",
    "cfg.data.unknown_commands_included = True\n",
    "cfg.data.silence_included = True\n",
    "cfg.data.unknown_binary_classification = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164442d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_UNKNOWN) as f:\n",
    "    cfg_un = json.load(f)\n",
    "\n",
    "cfg_un = Config(cfg_un)\n",
    "cfg_un.data.root = os.path.join('..', '..', 'data')\n",
    "\n",
    "cfg_un.data.unknown_commands_included = True\n",
    "cfg_un.data.silence_included = True\n",
    "cfg_un.data.unknown_binary_classification = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d7a7b91c97345b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:52.490958Z",
     "start_time": "2025-04-28T21:13:52.290491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes:  12\n",
      "Class balance in testing data:\n",
      "  down: 253\n",
      "  go: 251\n",
      "  left: 267\n",
      "  no: 252\n",
      "  off: 262\n",
      "  on: 246\n",
      "  right: 259\n",
      "  stop: 249\n",
      "  up: 272\n",
      "  yes: 256\n",
      "  _silence_: 39\n",
      "  _unknown_: 4268\n",
      "  unknown percentage: 62.09%\n",
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', '_silence_']\n",
      "num_classes:  12\n",
      "Class balance in testing data:\n",
      "  down: 253\n",
      "  go: 251\n",
      "  left: 267\n",
      "  no: 252\n",
      "  off: 262\n",
      "  on: 246\n",
      "  right: 259\n",
      "  stop: 249\n",
      "  up: 272\n",
      "  yes: 256\n",
      "  _silence_: 39\n",
      "  _unknown_: 4268\n",
      "  unknown percentage: 62.09%\n"
     ]
    }
   ],
   "source": [
    "from dataset.dataset import SpeechCommandsDataset\n",
    "\n",
    "test_dataset = SpeechCommandsDataset(\n",
    "    root_dir=cfg.data.root,\n",
    "    cfg=cfg,\n",
    "    mode=MODE\n",
    ")\n",
    "print(test_dataset.target_commands)\n",
    "\n",
    "\n",
    "test_dataset_un = SpeechCommandsDataset(\n",
    "    root_dir=cfg.data.root,\n",
    "    cfg=cfg_un,\n",
    "    mode=MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d8862e37d74b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:53.300975Z",
     "start_time": "2025-04-28T21:13:52.663892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modeling.model import build_model\n",
    "\n",
    "models_unknown = []\n",
    "for run_dir in run_dirs_2:\n",
    "    model_path = os.path.join(run_dir, 'best.pth')\n",
    "    if os.path.exists(model_path):\n",
    "        model = build_model(cfg_un, 2)\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state_dict)\n",
    "        models_unknown.append(model)\n",
    "    \n",
    "models_known = []\n",
    "for run_dir in run_dirs_11:\n",
    "    model_path = os.path.join(run_dir, 'best.pth')\n",
    "    if os.path.exists(model_path):\n",
    "        model = build_model(cfg, 11)\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(state_dict)\n",
    "        models_known.append(model)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd47baeaabb4ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:13:53.520160Z",
     "start_time": "2025-04-28T21:13:53.489807Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_and_plot_confusion_matrix_ensemble(models_unknown, models_known, test_dataset, test_dataset_un):\n",
    "    for model in models_known + models_unknown:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for idx in range(len(test_dataset)):\n",
    "        data, true_label = test_dataset[idx]\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # Average softmax outputs for known models\n",
    "        with torch.no_grad():\n",
    "            outputs_known = [torch.softmax(model(data_tensor), dim=1) for model in models_known]\n",
    "        avg_output_known = torch.mean(torch.stack(outputs_known), dim=0)\n",
    "\n",
    "        data, true_label = test_dataset_un[idx]\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # Average softmax outputs for unknown models\n",
    "        with torch.no_grad():\n",
    "            outputs_unknown = [torch.softmax(model(data_tensor), dim=1) for model in models_unknown]\n",
    "        avg_output_unknown = torch.mean(torch.stack(outputs_unknown), dim=0)\n",
    "\n",
    "        # Combine outputs and make predictions\n",
    "        if avg_output_known[0][10] < 0.5 and avg_output_unknown[0][1] > 0.5:\n",
    "            output = torch.cat((avg_output_known * 0., torch.tensor([[avg_output_unknown[0][1]]], device=device)), dim=1)\n",
    "        else:\n",
    "            output = torch.cat((avg_output_known, torch.tensor([[0.]], device=device)), dim=1)\n",
    "\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(true_labels) == np.array(predicted_labels))\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(test_dataset.label_mapping.keys()))\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical', values_format=\".2f\", ax=ax)\n",
    "    plt.gca().grid(False)\n",
    "    plt.title(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../../charts/{NAME}.png', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68dd28c03327d12b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:16:02.998136Z",
     "start_time": "2025-04-28T21:13:53.678155Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_and_plot_confusion_matrix_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels_unknown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_known\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset_un\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36mevaluate_and_plot_confusion_matrix_ensemble\u001b[0;34m(models_unknown, models_known, test_dataset, test_dataset_un)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Average softmax outputs for unknown models\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     outputs_unknown \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39msoftmax(model(data_tensor), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_unknown]\n\u001b[1;32m     29\u001b[0m avg_output_unknown \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(outputs_unknown), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Combine outputs and make predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Average softmax outputs for unknown models\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     outputs_unknown \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_tensor\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_unknown]\n\u001b[1;32m     29\u001b[0m avg_output_unknown \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack(outputs_unknown), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Combine outputs and make predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/timm/models/vision_transformer.py:853\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 853\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/timm/models/vision_transformer.py:834\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    832\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/timm/models/vision_transformer.py:170\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))))\n\u001b[0;32m--> 170\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/timm/layers/mlp.py:44\u001b[0m, in \u001b[0;36mMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_and_plot_confusion_matrix_ensemble(models_unknown, models_known, test_dataset, test_dataset_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4554e68c2dcb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:16:03.490036Z",
     "start_time": "2025-04-28T21:16:03.416316Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_and_display_wrong_predictions(model, test_dataset, n=5):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    indices = list(range(len(test_dataset)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    wrong_predictions = []\n",
    "\n",
    "    for idx in indices:\n",
    "        data, true_label = test_dataset[idx]\n",
    "        if true_label != 10:\n",
    "            continue\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(data_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predicted_label = torch.argmax(output, dim=1).item()\n",
    "            prediction_certainty = probabilities[0, predicted_label].item()\n",
    "\n",
    "        if predicted_label != true_label:\n",
    "            wrong_predictions.append((idx, true_label, predicted_label, prediction_certainty))\n",
    "\n",
    "        if len(wrong_predictions) >= n:\n",
    "            break\n",
    "\n",
    "    for idx, true_label, predicted_label, prediction_certainty in wrong_predictions:\n",
    "        print(f\"Sample Index: {idx}\")\n",
    "        print(f\"{true_label}, {predicted_label}, {prediction_certainty}\")\n",
    "\n",
    "        def get_key_by_value(dictionary, value):\n",
    "            return next((k for k, v in dictionary.items() if v == value), '')\n",
    "        true_label_name = get_key_by_value(test_dataset.label_mapping, true_label)\n",
    "        predicted_label_name = get_key_by_value(test_dataset.label_mapping, predicted_label)\n",
    "        print(f\"True Label: {true_label_name}, Predicted Label: {predicted_label_name}\")\n",
    "        print(f\"Prediction Certainty: {prediction_certainty:.2f}\")\n",
    "\n",
    "        waveform, _ = test_dataset.get_waveform(idx)\n",
    "        display(Audio(waveform, rate=cfg.data.sample_rate))\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee14f6d43e2a05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:16:15.504361Z",
     "start_time": "2025-04-28T21:16:03.499071Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_and_display_wrong_predictions(model, test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
