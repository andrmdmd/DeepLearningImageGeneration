# Deep Learning Speech Recognition

## ğŸš€ Introduction

In this project, we evaluated and compared the performance of three deep learning architectures -- the M5 CNN, Vision Transformer (ViT), and Conformer -- on a multi-class speech command classification task. We investigated the impact of key model hyperparameters, audio input representations, and dataset balancing techniques on classification accuracy. Based on our experimental results, we developed and proposed an ensemble approach combining multiple M5 CNN classifiers to enhance overall performance.

## ğŸ“‚ Folder Structure

```plaintext
ğŸ“¦deep-learning-speech-recognition
 â”œâ”€â”€ ğŸ“‚configs                # Configuration files for experiments
 â”‚   â”œâ”€â”€ ğŸ“„config_utils.py    # Utils for showing or saving configs
 â”‚   â””â”€â”€ ğŸ“„config.py          # Main configuration script
 â”œâ”€â”€ ğŸ“‚configuration          # Experiment-specific configuration files
 â”‚   â”œâ”€â”€ ğŸ“‚10_class_m5_sweep
 â”‚   â”œâ”€â”€ ğŸ“‚10_class_vit_repr
 â”‚   â”œâ”€â”€ ğŸ“‚10_class_vit_sweep
 â”‚   â”œâ”€â”€ ğŸ“‚binary_test
 â”‚   â”œâ”€â”€ ğŸ“‚conformer_scratch_size_config
 â”‚   â”œâ”€â”€ ğŸ“‚full_dataset_sweep
 â”‚   â”œâ”€â”€ ğŸ“‚multiclass_test
 â”‚   â”œâ”€â”€ ğŸ“‚run_optimal_configs
 â”‚   â”œâ”€â”€ ğŸ“‚sampling_strategy
 â”‚   â””â”€â”€ ğŸ“‚sweep_test
 â”œâ”€â”€ ğŸ“‚dataset                # Data loading and preprocessing modules
 â”‚   â””â”€â”€ ğŸ“„dataset.py         # Data loader and preprocessing scripts
 â”œâ”€â”€ ğŸ“‚modeling               # Model architecture definitions
 â”‚   â”œâ”€â”€ ğŸ“„loss.py            # Loss function
 â”‚   â””â”€â”€ ğŸ“„model.py           # All architecture classes
 â”œâ”€â”€ ğŸ“‚utils                  # Utility scripts for various tasks
 â”‚   â””â”€â”€ ğŸ“„metrics.py         # Performance metrics
 â”œâ”€â”€ ğŸ“‚engine                 # Training and validation engine
 â”‚   â”œâ”€â”€ ğŸ“„base_engine.py     # Base engine class
 â”‚   â”œâ”€â”€ ğŸ“„sweep_engine.py    # Sweep engine class
 â”‚   â””â”€â”€ ğŸ“„engine.py          # Training and validation loops
 â”œâ”€â”€ ğŸ“„.gitignore             # Specifies intentionally untracked files
 â”œâ”€â”€ ğŸ“„LICENSE                # License file
 â”œâ”€â”€ ğŸ“„README.md              # Project README
 â”œâ”€â”€ ğŸ“„linter.sh              # Code formatting script
 â”œâ”€â”€ ğŸ“„requirements.txt       # Dependencies
 â”œâ”€â”€ ğŸ“„main.py                # Main training script
 â””â”€â”€ ğŸ“„sweep.py               # Sweep training script
```

## âš™ï¸ Configuration

Experiment configurations are stored in the `configuration` directory. WandB sweeps are heavily utilized for hyperparameter tuning and experiment tracking. Reproducibility is maintained by setting random seeds in configuration files.

## ğŸ‹ï¸â€â™‚ï¸ Training

### Basic Usage

1.  **Set up the environment:**

    ```shell
    pip install uv
    uv sync
    ```

2.  **Run training scripts:**

    Example for training a Ensemble model:

    ```shell
    python3 sweep.py --config configuration/full_dataset_sweep/config_11_classes.json --model.architecture="M5" \
    ```

3.  **Run WandB sweeps:**

    Example to run a WandB sweep for data augmentation experiments:

    ```shell
    ./configuration/data_augmentation/run_sweeps.sh
    ```

    Ensure you are logged into your WandB account.