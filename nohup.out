/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: m076etgn with config:
wandb: 	sweep: {'training': {'lr': 0.0001, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project 'full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250425_220558-m076etgn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/sweeps/osr1k32w
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/runs/m076etgn
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: osr1k32w
Sweep URL: https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/sweeps/osr1k32w
Class balance in training data:
  no: 1853
  go: 1861
  _silence_: 320
  on: 1864
  right: 1852
  stop: 1885
  off: 1839
  up: 1843
  yes: 1860
  down: 1842
  left: 1839
  _unknown_: 32550
  unknown percentage: 63.32%
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
  _unknown_: 4221
  unknown percentage: 61.74%
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
  _unknown_: 4268
  unknown percentage: 62.09%
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0001                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: ensemble                                |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  ├── conformer                                                  |
| |  |  ├── input_dim: 80                                           |
| |  |  ├── num_heads: 4                                            |
| |  |  ├── num_layers: 16                                          |
| |  |  ├── depthwise_conv_kernel_size: 31                          |
| |  |  └── dropout: 0.1                                            |
| |  └── tcnn                                                       |
| |     ├── n_channel: 32                                           |
| |     ├── num_blocks: 4                                           |
| |     ├── kernel_size: 3                                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: True                            |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: Conformer                                            |
| |  └── tags: ['full dataset']                                     |
| ├── sweep                                                         |
| |  ├── name: Conformer                                            |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Full dataset on all models sweeps            |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: full_dataset_sweep_Conformer                     |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 51408
 - 📝 Validation: 6837
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 15568428
 - 🧊 Non-trainable: 0
 - 🤯 Total: 15568428
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: gpjslr8p with config:
wandb: 	sweep: {'training': {'lr': 0.0001, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project 'full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250425_224126-gpjslr8p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/sweeps/x66nbbwn
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/runs/gpjslr8p
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: x66nbbwn
Sweep URL: https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/sweeps/x66nbbwn
Class balance in training data:
  no: 1853
  go: 1861
  _silence_: 320
  on: 1864
  right: 1852
  stop: 1885
  off: 1839
  up: 1843
  yes: 1860
  down: 1842
  left: 1839
  _unknown_: 32550
  unknown percentage: 63.32%
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
  _unknown_: 4221
  unknown percentage: 61.74%
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
  _unknown_: 4268
  unknown percentage: 62.09%
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0001                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: ensemble                                |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  ├── conformer                                                  |
| |  |  ├── input_dim: 80                                           |
| |  |  ├── num_heads: 2                                            |
| |  |  ├── num_layers: 4                                           |
| |  |  ├── depthwise_conv_kernel_size: 31                          |
| |  |  └── dropout: 0.1                                            |
| |  └── tcnn                                                       |
| |     ├── n_channel: 32                                           |
| |     ├── num_blocks: 4                                           |
| |     ├── kernel_size: 3                                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: True                            |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: Conformer                                            |
| |  └── tags: ['full dataset']                                     |
| ├── sweep                                                         |
| |  ├── name: Conformer                                            |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Full dataset on all models sweeps            |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: full_dataset_sweep_Conformer                     |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 51408
 - 📝 Validation: 6837
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 3911340
 - 🧊 Non-trainable: 0
 - 🤯 Total: 3911340
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: yuvev1cs with config:
wandb: 	sweep: {'training': {'lr': 0.0001, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project 'full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_003229-yuvev1cs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/sweeps/vfoo4d0y
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/runs/yuvev1cs
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: vfoo4d0y
Sweep URL: https://wandb.ai/dl-2-mm-jd/Full%20dataset%20on%20all%20models%20sweeps/sweeps/vfoo4d0y
Class balance in training data:
  no: 1853
  go: 1861
  _silence_: 320
  on: 1864
  right: 1852
  stop: 1885
  off: 1839
  up: 1843
  yes: 1860
  down: 1842
  left: 1839
  _unknown_: 32550
  unknown percentage: 63.32%
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
  _unknown_: 4221
  unknown percentage: 61.74%
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
  _unknown_: 4268
  unknown percentage: 62.09%
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0001                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: ensemble                                |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  ├── conformer                                                  |
| |  |  ├── input_dim: 80                                           |
| |  |  ├── num_heads: 4                                            |
| |  |  ├── num_layers: 16                                          |
| |  |  ├── depthwise_conv_kernel_size: 31                          |
| |  |  └── dropout: 0.1                                            |
| |  └── tcnn                                                       |
| |     ├── n_channel: 32                                           |
| |     ├── num_blocks: 4                                           |
| |     ├── kernel_size: 3                                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: True                            |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: Conformer                                            |
| |  └── tags: ['full dataset']                                     |
| ├── sweep                                                         |
| |  ├── name: Conformer                                            |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Full dataset on all models sweeps            |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: full_dataset_sweep_Conformer                     |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 51408
 - 📝 Validation: 6837
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 4867532
 - 🧊 Non-trainable: 0
 - 🤯 Total: 4867532
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 7r0p4agb with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_133624-7r0p4agb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/7r0p4agb
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: n1m5gq7o
Sweep URL: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.359, loss: 1.993, precision: 0.441, recall: 0.342, f1: 0.340
new best found with: 0.539, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.539, loss: 1.667, precision: 0.615, recall: 0.509, f1: 0.515
train acc.: 0.621, loss: 1.379, precision: 0.658, recall: 0.596, f1: 0.606
new best found with: 0.688, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.688, loss: 1.199, precision: 0.745, recall: 0.657, f1: 0.673
train acc.: 0.722, loss: 1.027, precision: 0.748, recall: 0.706, f1: 0.718
new best found with: 0.721, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.721, loss: 0.962, precision: 0.777, recall: 0.700, f1: 0.718
train acc.: 0.772, loss: 0.822, precision: 0.789, recall: 0.762, f1: 0.772
new best found with: 0.794, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.794, loss: 0.761, precision: 0.832, recall: 0.792, f1: 0.804
train acc.: 0.808, loss: 0.694, precision: 0.819, recall: 0.800, f1: 0.807
new best found with: 0.817, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.817, loss: 0.669, precision: 0.842, recall: 0.815, f1: 0.824
train acc.: 0.832, loss: 0.605, precision: 0.839, recall: 0.823, f1: 0.829
new best found with: 0.822, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.822, loss: 0.622, precision: 0.849, recall: 0.822, f1: 0.830
train acc.: 0.846, loss: 0.543, precision: 0.853, recall: 0.839, f1: 0.845
new best found with: 0.824, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.824, loss: 0.574, precision: 0.851, recall: 0.827, f1: 0.833
train acc.: 0.858, loss: 0.495, precision: 0.864, recall: 0.851, f1: 0.857
new best found with: 0.836, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.836, loss: 0.533, precision: 0.860, recall: 0.836, f1: 0.844
train acc.: 0.870, loss: 0.458, precision: 0.875, recall: 0.863, f1: 0.868
new best found with: 0.842, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.842, loss: 0.515, precision: 0.862, recall: 0.844, f1: 0.850
train acc.: 0.875, loss: 0.427, precision: 0.881, recall: 0.868, f1: 0.873
val. acc.: 0.829, loss: 0.527, precision: 0.866, recall: 0.830, f1: 0.840
train acc.: 0.880, loss: 0.402, precision: 0.884, recall: 0.876, f1: 0.879
new best found with: 0.858, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.858, loss: 0.463, precision: 0.875, recall: 0.859, f1: 0.865
train acc.: 0.889, loss: 0.379, precision: 0.892, recall: 0.883, f1: 0.887
val. acc.: 0.855, loss: 0.475, precision: 0.871, recall: 0.857, f1: 0.862
train acc.: 0.894, loss: 0.362, precision: 0.896, recall: 0.887, f1: 0.891
val. acc.: 0.848, loss: 0.461, precision: 0.869, recall: 0.851, f1: 0.855
train acc.: 0.899, loss: 0.346, precision: 0.902, recall: 0.894, f1: 0.897
val. acc.: 0.856, loss: 0.461, precision: 0.874, recall: 0.857, f1: 0.863
train acc.: 0.899, loss: 0.335, precision: 0.903, recall: 0.894, f1: 0.898
new best found with: 0.865, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.865, loss: 0.440, precision: 0.882, recall: 0.870, f1: 0.874
train acc.: 0.906, loss: 0.315, precision: 0.910, recall: 0.900, f1: 0.904
val. acc.: 0.865, loss: 0.430, precision: 0.880, recall: 0.867, f1: 0.872
train acc.: 0.909, loss: 0.304, precision: 0.910, recall: 0.902, f1: 0.905
new best found with: 0.869, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.869, loss: 0.414, precision: 0.883, recall: 0.871, f1: 0.875
train acc.: 0.912, loss: 0.298, precision: 0.915, recall: 0.905, f1: 0.910
new best found with: 0.870, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.870, loss: 0.410, precision: 0.886, recall: 0.872, f1: 0.877
train acc.: 0.916, loss: 0.282, precision: 0.917, recall: 0.909, f1: 0.913
new best found with: 0.872, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.872, loss: 0.410, precision: 0.885, recall: 0.880, f1: 0.882
train acc.: 0.918, loss: 0.278, precision: 0.921, recall: 0.914, f1: 0.917
new best found with: 0.874, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.874, loss: 0.396, precision: 0.890, recall: 0.876, f1: 0.881
train acc.: 0.922, loss: 0.264, precision: 0.922, recall: 0.918, f1: 0.920
val. acc.: 0.866, loss: 0.415, precision: 0.886, recall: 0.868, f1: 0.874
train acc.: 0.922, loss: 0.260, precision: 0.923, recall: 0.916, f1: 0.919
val. acc.: 0.870, loss: 0.405, precision: 0.872, recall: 0.880, f1: 0.875
train acc.: 0.927, loss: 0.250, precision: 0.927, recall: 0.922, f1: 0.924
val. acc.: 0.872, loss: 0.403, precision: 0.887, recall: 0.874, f1: 0.879
train acc.: 0.930, loss: 0.242, precision: 0.931, recall: 0.927, f1: 0.929
val. acc.: 0.872, loss: 0.399, precision: 0.887, recall: 0.873, f1: 0.879
train acc.: 0.932, loss: 0.235, precision: 0.931, recall: 0.928, f1: 0.929
new best found with: 0.876, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.876, loss: 0.402, precision: 0.890, recall: 0.878, f1: 0.882
train acc.: 0.933, loss: 0.229, precision: 0.933, recall: 0.928, f1: 0.930
new best found with: 0.877, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.877, loss: 0.406, precision: 0.889, recall: 0.878, f1: 0.882
train acc.: 0.935, loss: 0.226, precision: 0.934, recall: 0.929, f1: 0.932
val. acc.: 0.872, loss: 0.394, precision: 0.886, recall: 0.874, f1: 0.879
train acc.: 0.937, loss: 0.217, precision: 0.936, recall: 0.933, f1: 0.934
val. acc.: 0.876, loss: 0.384, precision: 0.889, recall: 0.877, f1: 0.882
train acc.: 0.937, loss: 0.216, precision: 0.938, recall: 0.933, f1: 0.935
new best found with: 0.880, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.880, loss: 0.392, precision: 0.892, recall: 0.886, f1: 0.888
train acc.: 0.941, loss: 0.209, precision: 0.941, recall: 0.938, f1: 0.939
new best found with: 0.888, save to logs/11_classes_full_dataset_sweep_M5/run_2/checkpoint
val. acc.: 0.888, loss: 0.385, precision: 0.899, recall: 0.895, f1: 0.896
train acc.: 0.942, loss: 0.208, precision: 0.939, recall: 0.939, f1: 0.939
val. acc.: 0.881, loss: 0.391, precision: 0.895, recall: 0.886, f1: 0.889
train acc.: 0.945, loss: 0.198, precision: 0.945, recall: 0.941, f1: 0.943
val. acc.: 0.883, loss: 0.391, precision: 0.897, recall: 0.888, f1: 0.891
train acc.: 0.946, loss: 0.196, precision: 0.946, recall: 0.943, f1: 0.944
val. acc.: 0.885, loss: 0.381, precision: 0.896, recall: 0.890, f1: 0.893
train acc.: 0.945, loss: 0.195, precision: 0.942, recall: 0.942, f1: 0.942
val. acc.: 0.886, loss: 0.382, precision: 0.900, recall: 0.887, f1: 0.892
train acc.: 0.947, loss: 0.189, precision: 0.947, recall: 0.945, f1: 0.946
val. acc.: 0.881, loss: 0.387, precision: 0.893, recall: 0.888, f1: 0.890
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.886, loss: 0.374, precision: 0.895, recall: 0.890, f1: 0.892
Epoch 34/50 ━━━━━━━━━━━━━━━━━━━╸           68% 0:05:09 0:11:24 | best acc: 0.888wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇████████████████████
wandb:          acc/val ▁▄▅▆▇▇▇▇▇▇▇▇▇▇█████████████████████
wandb:          f1/test ▁
wandb:         f1/train ▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇████████████████████
wandb:           f1/val ▁▄▅▆▇▇▇▇▇▇▇▇▇▇█████████████████████
wandb:        loss/test ▁
wandb:       loss/train █▇▄▃▂▃▃▃▂▂▂▂▃▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁
wandb: loss/train_epoch █▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▄▅▆▇▇▇▇▇▇▇▇▇▇█████████████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████
wandb:    precision/val ▁▄▅▆▇▇▇▇▇▇▇▇▇▇███████▇█████████████
wandb:      recall/test ▁
wandb:     recall/train ▁▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████
wandb:       recall/val ▁▄▄▆▇▇▇▇▇▇▇▇▇▇█▇███████████████████
wandb:     scheduler_lr ████████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁
wandb:        time/data ▄▃▄▄▄▁▃▃▃▄▄▄▃▄▁▃▃▃▃▃▃▄▄▁▄▆▃▃▄▃▃▆▃█▃▄▃▃▃▃
wandb:        time/iter ▄▃▃▃▃▃▃▃▃▃▃█▃▁▃▃▄▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▆▃▃▃▃▆
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.88565
wandb:        acc/train 0.94718
wandb:          acc/val 0.88112
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.89211
wandb:         f1/train 0.94564
wandb:           f1/val 0.89007
wandb:        loss/test 0.3736
wandb:       loss/train 0.0776
wandb: loss/train_epoch 0.18924
wandb:         loss/val 0.38727
wandb:      max_acc/val 0.888
wandb:   precision/test 0.89548
wandb:  precision/train 0.94735
wandb:    precision/val 0.89338
wandb:      recall/test 0.89023
wandb:     recall/train 0.94456
wandb:       recall/val 0.88824
wandb:     scheduler_lr 7e-05
wandb:        time/data 0.0494
wandb:        time/iter 0.07213
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/7r0p4agb
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_133624-7r0p4agb/logs
wandb: Agent Starting Run: t4wi8xoz with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_134758-t4wi8xoz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/t4wi8xoz
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.357, loss: 1.992, precision: 0.438, recall: 0.340, f1: 0.337
new best found with: 0.541, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.541, loss: 1.655, precision: 0.616, recall: 0.510, f1: 0.515
train acc.: 0.619, loss: 1.377, precision: 0.657, recall: 0.595, f1: 0.605
new best found with: 0.687, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.687, loss: 1.195, precision: 0.742, recall: 0.657, f1: 0.672
train acc.: 0.723, loss: 1.022, precision: 0.749, recall: 0.707, f1: 0.719
new best found with: 0.726, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.726, loss: 0.955, precision: 0.780, recall: 0.703, f1: 0.721
train acc.: 0.775, loss: 0.817, precision: 0.791, recall: 0.765, f1: 0.774
new best found with: 0.791, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.791, loss: 0.771, precision: 0.827, recall: 0.786, f1: 0.799
train acc.: 0.809, loss: 0.688, precision: 0.819, recall: 0.801, f1: 0.808
new best found with: 0.818, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.818, loss: 0.663, precision: 0.838, recall: 0.815, f1: 0.823
train acc.: 0.831, loss: 0.602, precision: 0.838, recall: 0.823, f1: 0.829
new best found with: 0.823, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.823, loss: 0.612, precision: 0.849, recall: 0.823, f1: 0.832
train acc.: 0.847, loss: 0.541, precision: 0.855, recall: 0.839, f1: 0.846
new best found with: 0.835, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.835, loss: 0.553, precision: 0.858, recall: 0.836, f1: 0.843
train acc.: 0.858, loss: 0.493, precision: 0.864, recall: 0.852, f1: 0.857
new best found with: 0.846, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.846, loss: 0.521, precision: 0.861, recall: 0.848, f1: 0.853
train acc.: 0.871, loss: 0.456, precision: 0.875, recall: 0.865, f1: 0.869
val. acc.: 0.839, loss: 0.513, precision: 0.862, recall: 0.841, f1: 0.848
train acc.: 0.876, loss: 0.426, precision: 0.881, recall: 0.869, f1: 0.874
val. acc.: 0.837, loss: 0.511, precision: 0.868, recall: 0.838, f1: 0.846
train acc.: 0.883, loss: 0.398, precision: 0.888, recall: 0.878, f1: 0.882
new best found with: 0.857, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.857, loss: 0.467, precision: 0.874, recall: 0.858, f1: 0.864
train acc.: 0.891, loss: 0.377, precision: 0.895, recall: 0.886, f1: 0.890
val. acc.: 0.857, loss: 0.468, precision: 0.871, recall: 0.858, f1: 0.863
train acc.: 0.897, loss: 0.356, precision: 0.900, recall: 0.891, f1: 0.895
val. acc.: 0.851, loss: 0.464, precision: 0.875, recall: 0.852, f1: 0.859
train acc.: 0.899, loss: 0.342, precision: 0.902, recall: 0.895, f1: 0.898
new best found with: 0.859, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.859, loss: 0.460, precision: 0.875, recall: 0.860, f1: 0.865
train acc.: 0.903, loss: 0.331, precision: 0.907, recall: 0.897, f1: 0.901
new best found with: 0.866, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.866, loss: 0.440, precision: 0.884, recall: 0.868, f1: 0.874
train acc.: 0.908, loss: 0.313, precision: 0.911, recall: 0.902, f1: 0.906
val. acc.: 0.859, loss: 0.438, precision: 0.876, recall: 0.862, f1: 0.867
train acc.: 0.910, loss: 0.302, precision: 0.912, recall: 0.904, f1: 0.908
new best found with: 0.870, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.870, loss: 0.416, precision: 0.886, recall: 0.872, f1: 0.877
train acc.: 0.912, loss: 0.294, precision: 0.916, recall: 0.905, f1: 0.910
new best found with: 0.876, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.876, loss: 0.406, precision: 0.892, recall: 0.878, f1: 0.883
train acc.: 0.916, loss: 0.280, precision: 0.917, recall: 0.910, f1: 0.913
val. acc.: 0.871, loss: 0.406, precision: 0.883, recall: 0.877, f1: 0.879
train acc.: 0.918, loss: 0.276, precision: 0.921, recall: 0.914, f1: 0.917
new best found with: 0.880, save to logs/11_classes_full_dataset_sweep_M5/run_3/checkpoint
val. acc.: 0.880, loss: 0.390, precision: 0.893, recall: 0.885, f1: 0.888
train acc.: 0.924, loss: 0.262, precision: 0.924, recall: 0.919, f1: 0.921
val. acc.: 0.873, loss: 0.414, precision: 0.892, recall: 0.874, f1: 0.880
train acc.: 0.923, loss: 0.257, precision: 0.925, recall: 0.918, f1: 0.921
val. acc.: 0.869, loss: 0.403, precision: 0.870, recall: 0.878, f1: 0.872
train acc.: 0.928, loss: 0.247, precision: 0.929, recall: 0.924, f1: 0.926
val. acc.: 0.876, loss: 0.390, precision: 0.890, recall: 0.878, f1: 0.883
train acc.: 0.930, loss: 0.240, precision: 0.931, recall: 0.926, f1: 0.928
val. acc.: 0.880, loss: 0.386, precision: 0.893, recall: 0.881, f1: 0.886
train acc.: 0.934, loss: 0.231, precision: 0.933, recall: 0.929, f1: 0.931
val. acc.: 0.878, loss: 0.390, precision: 0.893, recall: 0.880, f1: 0.885
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.876, loss: 0.407, precision: 0.890, recall: 0.879, f1: 0.884
Epoch 24/50 ━━━━━━━━━━━━━╸                 48% 0:08:19 0:08:05 | best acc: 0.880wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▄▅▆▆▇▇▇▇▇▇▇█████████████
wandb:          acc/val ▁▄▅▆▇▇▇▇▇▇██▇████████████
wandb:          f1/test ▁
wandb:         f1/train ▁▄▆▆▇▇▇▇▇▇▇██████████████
wandb:           f1/val ▁▄▅▆▇▇▇▇▇▇██▇████████████
wandb:        loss/test ▁
wandb:       loss/train █▆▇▆▆▅▅▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▂▁▂▁▁▁
wandb: loss/train_epoch █▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▄▅▆▇▇▇▇▇▇███████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▄▅▆▆▇▇▇▇▇▇▇█████████████
wandb:    precision/val ▁▄▅▆▇▇▇▇▇▇█▇█████████▇███
wandb:      recall/test ▁
wandb:     recall/train ▁▄▅▆▆▇▇▇▇▇▇▇█████████████
wandb:       recall/val ▁▄▅▆▇▇▇▇▇▇█▇▇████████████
wandb:     scheduler_lr ██████▇▇▇▇▇▆▆▆▅▅▅▄▄▃▃▂▂▁▁
wandb:        time/data ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        time/iter █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.87606
wandb:        acc/train 0.93419
wandb:          acc/val 0.87844
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.88359
wandb:         f1/train 0.931
wandb:           f1/val 0.88461
wandb:        loss/test 0.40658
wandb:       loss/train 0.22944
wandb: loss/train_epoch 0.23091
wandb:         loss/val 0.39016
wandb:      max_acc/val 0.87997
wandb:   precision/test 0.89006
wandb:  precision/train 0.93334
wandb:    precision/val 0.89295
wandb:      recall/test 0.8795
wandb:     recall/train 0.92923
wandb:       recall/val 0.88021
wandb:     scheduler_lr 0.00016
wandb:        time/data 0.04314
wandb:        time/iter 0.06626
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/t4wi8xoz
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_134758-t4wi8xoz/logs
wandb: Agent Starting Run: 8sbub8ye with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_135611-8sbub8ye
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/8sbub8ye
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.359, loss: 1.993, precision: 0.441, recall: 0.342, f1: 0.340
new best found with: 0.539, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.539, loss: 1.667, precision: 0.615, recall: 0.509, f1: 0.515
train acc.: 0.621, loss: 1.379, precision: 0.659, recall: 0.597, f1: 0.607
new best found with: 0.668, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.668, loss: 1.214, precision: 0.732, recall: 0.639, f1: 0.654
train acc.: 0.720, loss: 1.026, precision: 0.746, recall: 0.704, f1: 0.716
new best found with: 0.737, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.737, loss: 0.932, precision: 0.782, recall: 0.717, f1: 0.734
train acc.: 0.774, loss: 0.821, precision: 0.792, recall: 0.763, f1: 0.774
new best found with: 0.806, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.806, loss: 0.753, precision: 0.829, recall: 0.805, f1: 0.814
train acc.: 0.808, loss: 0.693, precision: 0.818, recall: 0.800, f1: 0.807
new best found with: 0.817, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.817, loss: 0.671, precision: 0.843, recall: 0.812, f1: 0.822
train acc.: 0.830, loss: 0.604, precision: 0.837, recall: 0.822, f1: 0.828
new best found with: 0.828, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.828, loss: 0.604, precision: 0.849, recall: 0.829, f1: 0.836
train acc.: 0.847, loss: 0.541, precision: 0.854, recall: 0.841, f1: 0.846
new best found with: 0.831, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.831, loss: 0.557, precision: 0.858, recall: 0.834, f1: 0.842
train acc.: 0.859, loss: 0.492, precision: 0.866, recall: 0.853, f1: 0.858
new best found with: 0.845, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.845, loss: 0.524, precision: 0.866, recall: 0.847, f1: 0.854
train acc.: 0.871, loss: 0.453, precision: 0.876, recall: 0.864, f1: 0.869
new best found with: 0.848, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.848, loss: 0.505, precision: 0.869, recall: 0.850, f1: 0.857
train acc.: 0.875, loss: 0.425, precision: 0.881, recall: 0.869, f1: 0.874
val. acc.: 0.837, loss: 0.519, precision: 0.872, recall: 0.840, f1: 0.848
train acc.: 0.880, loss: 0.397, precision: 0.885, recall: 0.875, f1: 0.879
val. acc.: 0.841, loss: 0.500, precision: 0.868, recall: 0.842, f1: 0.850
train acc.: 0.889, loss: 0.375, precision: 0.893, recall: 0.883, f1: 0.887
new best found with: 0.856, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.856, loss: 0.473, precision: 0.872, recall: 0.857, f1: 0.863
train acc.: 0.894, loss: 0.357, precision: 0.897, recall: 0.887, f1: 0.891
new best found with: 0.859, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.859, loss: 0.445, precision: 0.877, recall: 0.860, f1: 0.866
train acc.: 0.899, loss: 0.341, precision: 0.903, recall: 0.894, f1: 0.898
new best found with: 0.862, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.862, loss: 0.442, precision: 0.878, recall: 0.865, f1: 0.869
train acc.: 0.900, loss: 0.330, precision: 0.903, recall: 0.894, f1: 0.898
val. acc.: 0.857, loss: 0.449, precision: 0.876, recall: 0.858, f1: 0.864
train acc.: 0.908, loss: 0.312, precision: 0.911, recall: 0.900, f1: 0.905
val. acc.: 0.856, loss: 0.447, precision: 0.874, recall: 0.857, f1: 0.863
train acc.: 0.908, loss: 0.301, precision: 0.909, recall: 0.903, f1: 0.906
new best found with: 0.866, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.866, loss: 0.412, precision: 0.886, recall: 0.869, f1: 0.875
train acc.: 0.911, loss: 0.293, precision: 0.914, recall: 0.904, f1: 0.908
new best found with: 0.875, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.875, loss: 0.403, precision: 0.890, recall: 0.876, f1: 0.881
train acc.: 0.916, loss: 0.278, precision: 0.917, recall: 0.910, f1: 0.913
val. acc.: 0.864, loss: 0.418, precision: 0.879, recall: 0.871, f1: 0.873
train acc.: 0.916, loss: 0.274, precision: 0.919, recall: 0.912, f1: 0.915
val. acc.: 0.870, loss: 0.399, precision: 0.889, recall: 0.872, f1: 0.878
train acc.: 0.922, loss: 0.259, precision: 0.922, recall: 0.917, f1: 0.919
new best found with: 0.878, save to logs/11_classes_full_dataset_sweep_M5/run_4/checkpoint
val. acc.: 0.878, loss: 0.400, precision: 0.893, recall: 0.880, f1: 0.885
train acc.: 0.924, loss: 0.256, precision: 0.925, recall: 0.918, f1: 0.921
val. acc.: 0.877, loss: 0.406, precision: 0.883, recall: 0.886, f1: 0.883
train acc.: 0.926, loss: 0.246, precision: 0.925, recall: 0.922, f1: 0.923
val. acc.: 0.872, loss: 0.403, precision: 0.882, recall: 0.880, f1: 0.880
train acc.: 0.929, loss: 0.239, precision: 0.928, recall: 0.926, f1: 0.926
val. acc.: 0.878, loss: 0.396, precision: 0.889, recall: 0.884, f1: 0.886
train acc.: 0.933, loss: 0.227, precision: 0.933, recall: 0.929, f1: 0.931
val. acc.: 0.873, loss: 0.401, precision: 0.890, recall: 0.879, f1: 0.883
train acc.: 0.932, loss: 0.226, precision: 0.932, recall: 0.928, f1: 0.930
val. acc.: 0.878, loss: 0.406, precision: 0.894, recall: 0.879, f1: 0.884
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.880, loss: 0.401, precision: 0.895, recall: 0.873, f1: 0.882
Epoch 25/50 ━━━━━━━━━━━━━━╸                50% 0:08:01 0:08:24 | best acc: 0.878wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▄▅▆▆▇▇▇▇▇▇▇██████████████
wandb:          acc/val ▁▄▅▇▇▇▇▇▇▇▇███████████████
wandb:          f1/test ▁
wandb:         f1/train ▁▄▅▆▇▇▇▇▇▇▇▇██████████████
wandb:           f1/val ▁▄▅▇▇▇▇▇▇▇▇███████████████
wandb:        loss/test ▁
wandb:       loss/train █▅▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▂▁▁▂▁▂▁▁▂▁▁▁▁
wandb: loss/train_epoch █▆▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▄▅▇▇▇▇▇▇▇▇███████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▄▅▆▆▇▇▇▇▇▇▇▇█████████████
wandb:    precision/val ▁▄▅▆▇▇▇▇▇▇▇▇███▇██████████
wandb:      recall/test ▁
wandb:     recall/train ▁▄▅▆▆▇▇▇▇▇▇▇██████████████
wandb:       recall/val ▁▃▅▆▇▇▇▇▇▇▇▇██▇▇██████████
wandb:        time/data ▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁
wandb:        time/iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.87989
wandb:        acc/train 0.93191
wandb:          acc/val 0.87768
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.88153
wandb:         f1/train 0.92967
wandb:           f1/val 0.88439
wandb:        loss/test 0.40068
wandb:       loss/train 0.099
wandb: loss/train_epoch 0.22619
wandb:         loss/val 0.40556
wandb:      max_acc/val 0.87844
wandb:   precision/test 0.89505
wandb:  precision/train 0.93232
wandb:    precision/val 0.89367
wandb:      recall/test 0.87321
wandb:     recall/train 0.92759
wandb:       recall/val 0.87911
wandb:        time/data 0.04664
wandb:        time/iter 0.07099
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/8sbub8ye
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_135611-8sbub8ye/logs
wandb: Agent Starting Run: 00z0xsj6 with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_140444-00z0xsj6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/00z0xsj6
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.357, loss: 1.992, precision: 0.438, recall: 0.340, f1: 0.337
new best found with: 0.541, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.541, loss: 1.655, precision: 0.616, recall: 0.510, f1: 0.515
train acc.: 0.620, loss: 1.377, precision: 0.657, recall: 0.595, f1: 0.606
new best found with: 0.659, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.659, loss: 1.213, precision: 0.729, recall: 0.630, f1: 0.646
train acc.: 0.725, loss: 1.021, precision: 0.750, recall: 0.708, f1: 0.720
new best found with: 0.748, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.748, loss: 0.922, precision: 0.783, recall: 0.726, f1: 0.741
train acc.: 0.776, loss: 0.815, precision: 0.792, recall: 0.767, f1: 0.776
new best found with: 0.785, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.785, loss: 0.770, precision: 0.825, recall: 0.783, f1: 0.796
train acc.: 0.811, loss: 0.686, precision: 0.822, recall: 0.803, f1: 0.810
new best found with: 0.823, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.823, loss: 0.653, precision: 0.840, recall: 0.819, f1: 0.827
train acc.: 0.832, loss: 0.599, precision: 0.840, recall: 0.824, f1: 0.830
new best found with: 0.826, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.826, loss: 0.610, precision: 0.849, recall: 0.825, f1: 0.833
train acc.: 0.848, loss: 0.536, precision: 0.855, recall: 0.842, f1: 0.847
new best found with: 0.841, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.841, loss: 0.544, precision: 0.863, recall: 0.843, f1: 0.850
train acc.: 0.860, loss: 0.489, precision: 0.865, recall: 0.854, f1: 0.859
val. acc.: 0.830, loss: 0.539, precision: 0.853, recall: 0.827, f1: 0.836
train acc.: 0.870, loss: 0.453, precision: 0.874, recall: 0.863, f1: 0.868
val. acc.: 0.841, loss: 0.507, precision: 0.858, recall: 0.843, f1: 0.849
train acc.: 0.876, loss: 0.422, precision: 0.880, recall: 0.868, f1: 0.873
val. acc.: 0.833, loss: 0.517, precision: 0.868, recall: 0.835, f1: 0.844
train acc.: 0.884, loss: 0.396, precision: 0.888, recall: 0.879, f1: 0.883
new best found with: 0.860, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.860, loss: 0.464, precision: 0.875, recall: 0.861, f1: 0.866
train acc.: 0.890, loss: 0.375, precision: 0.894, recall: 0.885, f1: 0.888
val. acc.: 0.859, loss: 0.473, precision: 0.875, recall: 0.861, f1: 0.865
train acc.: 0.897, loss: 0.353, precision: 0.900, recall: 0.891, f1: 0.895
new best found with: 0.860, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.860, loss: 0.446, precision: 0.865, recall: 0.869, f1: 0.865
train acc.: 0.900, loss: 0.341, precision: 0.904, recall: 0.895, f1: 0.899
val. acc.: 0.836, loss: 0.507, precision: 0.860, recall: 0.841, f1: 0.845
train acc.: 0.903, loss: 0.324, precision: 0.906, recall: 0.898, f1: 0.901
new best found with: 0.861, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.861, loss: 0.443, precision: 0.881, recall: 0.862, f1: 0.868
train acc.: 0.908, loss: 0.310, precision: 0.911, recall: 0.901, f1: 0.905
new best found with: 0.870, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.870, loss: 0.423, precision: 0.884, recall: 0.872, f1: 0.877
train acc.: 0.911, loss: 0.298, precision: 0.913, recall: 0.905, f1: 0.909
new best found with: 0.872, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.872, loss: 0.411, precision: 0.889, recall: 0.871, f1: 0.878
train acc.: 0.911, loss: 0.293, precision: 0.913, recall: 0.904, f1: 0.908
val. acc.: 0.868, loss: 0.412, precision: 0.885, recall: 0.870, f1: 0.875
train acc.: 0.918, loss: 0.277, precision: 0.918, recall: 0.911, f1: 0.914
val. acc.: 0.863, loss: 0.412, precision: 0.874, recall: 0.869, f1: 0.869
train acc.: 0.918, loss: 0.272, precision: 0.920, recall: 0.914, f1: 0.917
new best found with: 0.881, save to logs/11_classes_full_dataset_sweep_M5/run_5/checkpoint
val. acc.: 0.881, loss: 0.383, precision: 0.896, recall: 0.882, f1: 0.887
train acc.: 0.922, loss: 0.260, precision: 0.920, recall: 0.917, f1: 0.918
val. acc.: 0.872, loss: 0.402, precision: 0.890, recall: 0.873, f1: 0.879
train acc.: 0.924, loss: 0.257, precision: 0.924, recall: 0.917, f1: 0.920
val. acc.: 0.867, loss: 0.401, precision: 0.882, recall: 0.875, f1: 0.877
train acc.: 0.928, loss: 0.244, precision: 0.929, recall: 0.923, f1: 0.925
val. acc.: 0.873, loss: 0.400, precision: 0.890, recall: 0.875, f1: 0.880
train acc.: 0.931, loss: 0.234, precision: 0.930, recall: 0.928, f1: 0.929
val. acc.: 0.878, loss: 0.383, precision: 0.890, recall: 0.880, f1: 0.884
train acc.: 0.931, loss: 0.228, precision: 0.930, recall: 0.927, f1: 0.929
val. acc.: 0.862, loss: 0.408, precision: 0.884, recall: 0.865, f1: 0.871
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.875, loss: 0.402, precision: 0.892, recall: 0.871, f1: 0.879
Epoch 24/50 ━━━━━━━━━━━━━╸                 48% 0:08:26 0:08:05 | best acc: 0.881wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▄▅▆▇▇▇▇▇▇▇▇█████████████
wandb:          acc/val ▁▃▅▆▇▇▇▇▇▇███▇███████████
wandb:          f1/test ▁
wandb:         f1/train ▁▄▆▆▇▇▇▇▇▇▇██████████████
wandb:           f1/val ▁▃▅▆▇▇▇▇▇▇███▇███████████
wandb:        loss/test ▁
wandb:       loss/train █▇▅▄▃▂▂▃▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁
wandb: loss/train_epoch █▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▆▄▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▃▅▆▇▇▇▇▇▇███████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▄▅▆▆▇▇▇▇▇▇▇█████████████
wandb:    precision/val ▁▄▅▆▇▇▇▇▇▇▇▇▇▇████▇██████
wandb:      recall/test ▁
wandb:     recall/train ▁▄▅▆▇▇▇▇▇▇▇▇█████████████
wandb:       recall/val ▁▃▅▆▇▇▇▇▇▇███▇███████████
wandb:        time/data ▆▆▆▆▇▇▆▆▇▇▆▆▇█▆▆▇▆▇▆▇▇▆█▆▆█▇▆▆▇▇▆▇▇▅▆▇▁▇
wandb:        time/iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.87529
wandb:        acc/train 0.93096
wandb:          acc/val 0.862
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.87893
wandb:         f1/train 0.92865
wandb:           f1/val 0.87084
wandb:        loss/test 0.40218
wandb:       loss/train 0.30024
wandb: loss/train_epoch 0.22839
wandb:         loss/val 0.40825
wandb:      max_acc/val 0.88112
wandb:   precision/test 0.89158
wandb:  precision/train 0.93034
wandb:    precision/val 0.88443
wandb:      recall/test 0.87086
wandb:     recall/train 0.92741
wandb:       recall/val 0.86465
wandb:        time/data 0.05584
wandb:        time/iter 0.07892
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/00z0xsj6
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_140444-00z0xsj6/logs
wandb: Agent Starting Run: 41xgaf87 with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_141258-41xgaf87
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/41xgaf87
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.001                                                  |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.514, loss: 1.577, precision: 0.568, recall: 0.497, f1: 0.507
new best found with: 0.722, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.722, loss: 1.024, precision: 0.767, recall: 0.703, f1: 0.719
train acc.: 0.766, loss: 0.812, precision: 0.780, recall: 0.758, f1: 0.766
new best found with: 0.779, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.779, loss: 0.717, precision: 0.819, recall: 0.780, f1: 0.790
train acc.: 0.825, loss: 0.584, precision: 0.834, recall: 0.820, f1: 0.826
val. acc.: 0.744, loss: 0.751, precision: 0.824, recall: 0.756, f1: 0.766
train acc.: 0.858, loss: 0.480, precision: 0.862, recall: 0.852, f1: 0.856
new best found with: 0.807, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.807, loss: 0.572, precision: 0.850, recall: 0.812, f1: 0.821
train acc.: 0.870, loss: 0.424, precision: 0.875, recall: 0.864, f1: 0.869
new best found with: 0.850, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.850, loss: 0.471, precision: 0.874, recall: 0.852, f1: 0.858
train acc.: 0.887, loss: 0.375, precision: 0.889, recall: 0.881, f1: 0.885
new best found with: 0.853, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.853, loss: 0.468, precision: 0.877, recall: 0.849, f1: 0.858
train acc.: 0.894, loss: 0.345, precision: 0.897, recall: 0.889, f1: 0.893
new best found with: 0.853, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.853, loss: 0.439, precision: 0.873, recall: 0.852, f1: 0.859
train acc.: 0.904, loss: 0.315, precision: 0.906, recall: 0.899, f1: 0.902
new best found with: 0.871, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.871, loss: 0.415, precision: 0.887, recall: 0.871, f1: 0.877
train acc.: 0.909, loss: 0.296, precision: 0.910, recall: 0.903, f1: 0.906
new best found with: 0.876, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.876, loss: 0.389, precision: 0.891, recall: 0.877, f1: 0.882
train acc.: 0.913, loss: 0.276, precision: 0.914, recall: 0.909, f1: 0.911
new best found with: 0.883, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.883, loss: 0.384, precision: 0.897, recall: 0.882, f1: 0.887
train acc.: 0.918, loss: 0.260, precision: 0.916, recall: 0.914, f1: 0.915
new best found with: 0.885, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.885, loss: 0.363, precision: 0.895, recall: 0.887, f1: 0.889
train acc.: 0.925, loss: 0.239, precision: 0.925, recall: 0.922, f1: 0.924
val. acc.: 0.880, loss: 0.387, precision: 0.889, recall: 0.890, f1: 0.887
train acc.: 0.929, loss: 0.228, precision: 0.928, recall: 0.927, f1: 0.927
val. acc.: 0.839, loss: 0.502, precision: 0.836, recall: 0.849, f1: 0.826
train acc.: 0.931, loss: 0.222, precision: 0.925, recall: 0.927, f1: 0.926
val. acc.: 0.872, loss: 0.412, precision: 0.865, recall: 0.874, f1: 0.865
train acc.: 0.934, loss: 0.215, precision: 0.933, recall: 0.933, f1: 0.933
val. acc.: 0.880, loss: 0.371, precision: 0.894, recall: 0.883, f1: 0.887
train acc.: 0.939, loss: 0.198, precision: 0.936, recall: 0.937, f1: 0.936
new best found with: 0.900, save to logs/11_classes_full_dataset_sweep_M5/run_6/checkpoint
val. acc.: 0.900, loss: 0.333, precision: 0.898, recall: 0.908, f1: 0.901
train acc.: 0.943, loss: 0.187, precision: 0.941, recall: 0.943, f1: 0.942
val. acc.: 0.891, loss: 0.362, precision: 0.901, recall: 0.900, f1: 0.900
train acc.: 0.945, loss: 0.180, precision: 0.940, recall: 0.945, f1: 0.942
val. acc.: 0.888, loss: 0.349, precision: 0.900, recall: 0.896, f1: 0.896
train acc.: 0.946, loss: 0.172, precision: 0.940, recall: 0.946, f1: 0.943
val. acc.: 0.886, loss: 0.366, precision: 0.878, recall: 0.895, f1: 0.883
train acc.: 0.947, loss: 0.167, precision: 0.942, recall: 0.949, f1: 0.945
val. acc.: 0.897, loss: 0.348, precision: 0.888, recall: 0.905, f1: 0.894
train acc.: 0.952, loss: 0.156, precision: 0.947, recall: 0.954, f1: 0.950
val. acc.: 0.899, loss: 0.333, precision: 0.882, recall: 0.907, f1: 0.889
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.896, loss: 0.352, precision: 0.885, recall: 0.902, f1: 0.889
Epoch 20/50 ━━━━━━━━━━━╸                   40% 0:09:44 0:06:49 | best acc: 0.900wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇▇▇▇▇██████████
wandb:          acc/val ▁▃▂▄▆▆▆▇▇▇▇▇▆▇▇███▇██
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇▇▇▇▇██████████
wandb:           f1/val ▁▄▃▅▆▆▆▇▇▇█▇▅▇▇███▇██
wandb:        loss/test ▁
wandb:       loss/train █▇▇▇▄▅▆▄▄▄▃▃▄▃▂▃▃▂▃▂▁▂▁▃▂▃▂▂▃▃▂▂▂▂▁▂▃▂▁▁
wandb: loss/train_epoch █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▅▃▂▂▂▂▂▂▁▂▃▂▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▃▃▄▆▆▆▇▇▇▇▇▇▇▇██████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▆▇▇▇▇▇▇▇██████████
wandb:    precision/val ▁▄▄▅▇▇▇▇▇██▇▅▆████▇▇▇
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▆▇▇▇▇▇▇▇██████████
wandb:       recall/val ▁▄▃▅▆▆▆▇▇▇▇▇▆▇▇██████
wandb:     scheduler_lr ██████▇▇▇▆▆▆▅▅▄▄▃▃▂▂▁
wandb:        time/data ▁▆▇▆▆▆▅▅▆▆▅▆▅▆▆▆▆▆▆▅▆█▆▅▂▅▆▅▆▅▆▆▁▁▇▆▅▆▆▆
wandb:        time/iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁▂▂▁▁▃▁▁▂▄▁▂▁▁▅█▁▁▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.89563
wandb:        acc/train 0.95222
wandb:          acc/val 0.89946
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.88942
wandb:         f1/train 0.9498
wandb:           f1/val 0.88897
wandb:        loss/test 0.35185
wandb:       loss/train 0.11038
wandb: loss/train_epoch 0.15597
wandb:         loss/val 0.33335
wandb:      max_acc/val 0.89985
wandb:   precision/test 0.88523
wandb:  precision/train 0.94672
wandb:    precision/val 0.88204
wandb:      recall/test 0.90153
wandb:     recall/train 0.95367
wandb:       recall/val 0.90715
wandb:     scheduler_lr 0.00065
wandb:        time/data 0.04459
wandb:        time/iter 0.06714
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/41xgaf87
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_141258-41xgaf87/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: n7xtozhx with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_142003-n7xtozhx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/n7xtozhx
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.001                                                  |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.521, loss: 1.560, precision: 0.569, recall: 0.503, f1: 0.512
new best found with: 0.711, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.711, loss: 1.047, precision: 0.770, recall: 0.707, f1: 0.721
train acc.: 0.773, loss: 0.794, precision: 0.786, recall: 0.766, f1: 0.774
new best found with: 0.773, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.773, loss: 0.722, precision: 0.823, recall: 0.773, f1: 0.784
train acc.: 0.832, loss: 0.569, precision: 0.842, recall: 0.827, f1: 0.833
new best found with: 0.818, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.818, loss: 0.577, precision: 0.858, recall: 0.816, f1: 0.828
train acc.: 0.861, loss: 0.468, precision: 0.865, recall: 0.856, f1: 0.860
new best found with: 0.821, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.821, loss: 0.534, precision: 0.859, recall: 0.815, f1: 0.829
train acc.: 0.874, loss: 0.418, precision: 0.878, recall: 0.867, f1: 0.871
new best found with: 0.854, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.854, loss: 0.469, precision: 0.881, recall: 0.856, f1: 0.863
train acc.: 0.888, loss: 0.364, precision: 0.890, recall: 0.882, f1: 0.885
new best found with: 0.855, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.855, loss: 0.451, precision: 0.873, recall: 0.857, f1: 0.861
train acc.: 0.895, loss: 0.338, precision: 0.898, recall: 0.890, f1: 0.894
val. acc.: 0.831, loss: 0.506, precision: 0.818, recall: 0.844, f1: 0.806
train acc.: 0.903, loss: 0.308, precision: 0.903, recall: 0.900, f1: 0.901
new best found with: 0.881, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.881, loss: 0.387, precision: 0.868, recall: 0.889, f1: 0.875
train acc.: 0.910, loss: 0.290, precision: 0.911, recall: 0.906, f1: 0.908
new best found with: 0.885, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.885, loss: 0.374, precision: 0.896, recall: 0.884, f1: 0.889
train acc.: 0.915, loss: 0.273, precision: 0.917, recall: 0.910, f1: 0.913
val. acc.: 0.871, loss: 0.411, precision: 0.888, recall: 0.872, f1: 0.876
train acc.: 0.922, loss: 0.251, precision: 0.920, recall: 0.919, f1: 0.919
new best found with: 0.885, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.885, loss: 0.368, precision: 0.868, recall: 0.889, f1: 0.873
train acc.: 0.926, loss: 0.237, precision: 0.925, recall: 0.926, f1: 0.925
new best found with: 0.891, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.891, loss: 0.346, precision: 0.901, recall: 0.896, f1: 0.897
train acc.: 0.932, loss: 0.220, precision: 0.928, recall: 0.930, f1: 0.929
val. acc.: 0.885, loss: 0.368, precision: 0.864, recall: 0.893, f1: 0.869
train acc.: 0.931, loss: 0.215, precision: 0.927, recall: 0.930, f1: 0.928
val. acc.: 0.877, loss: 0.380, precision: 0.895, recall: 0.877, f1: 0.883
train acc.: 0.933, loss: 0.210, precision: 0.932, recall: 0.933, f1: 0.932
val. acc.: 0.889, loss: 0.352, precision: 0.900, recall: 0.898, f1: 0.898
train acc.: 0.940, loss: 0.200, precision: 0.937, recall: 0.939, f1: 0.938
val. acc.: 0.891, loss: 0.354, precision: 0.904, recall: 0.896, f1: 0.898
train acc.: 0.943, loss: 0.185, precision: 0.941, recall: 0.943, f1: 0.941
new best found with: 0.900, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.900, loss: 0.316, precision: 0.901, recall: 0.908, f1: 0.903
train acc.: 0.945, loss: 0.178, precision: 0.941, recall: 0.947, f1: 0.943
val. acc.: 0.896, loss: 0.328, precision: 0.895, recall: 0.904, f1: 0.898
train acc.: 0.950, loss: 0.166, precision: 0.945, recall: 0.950, f1: 0.947
val. acc.: 0.887, loss: 0.352, precision: 0.867, recall: 0.896, f1: 0.874
train acc.: 0.950, loss: 0.162, precision: 0.947, recall: 0.951, f1: 0.948
new best found with: 0.903, save to logs/11_classes_full_dataset_sweep_M5/run_7/checkpoint
val. acc.: 0.903, loss: 0.327, precision: 0.908, recall: 0.911, f1: 0.909
train acc.: 0.953, loss: 0.154, precision: 0.949, recall: 0.954, f1: 0.951
val. acc.: 0.893, loss: 0.348, precision: 0.904, recall: 0.901, f1: 0.899
train acc.: 0.956, loss: 0.146, precision: 0.951, recall: 0.956, f1: 0.953
val. acc.: 0.900, loss: 0.339, precision: 0.910, recall: 0.908, f1: 0.908
train acc.: 0.959, loss: 0.138, precision: 0.956, recall: 0.961, f1: 0.958
val. acc.: 0.899, loss: 0.322, precision: 0.888, recall: 0.907, f1: 0.894
train acc.: 0.959, loss: 0.137, precision: 0.955, recall: 0.960, f1: 0.957
val. acc.: 0.898, loss: 0.334, precision: 0.893, recall: 0.906, f1: 0.896
train acc.: 0.960, loss: 0.131, precision: 0.957, recall: 0.962, f1: 0.959
val. acc.: 0.892, loss: 0.341, precision: 0.909, recall: 0.899, f1: 0.902
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.913, loss: 0.308, precision: 0.917, recall: 0.919, f1: 0.918
Epoch 24/50 ━━━━━━━━━━━━━╸                 48% 0:08:25 0:07:57 | best acc: 0.903wandb: uploading output.log; uploading wandb-summary.json; uploading history steps 7190-7375, summary, console lines 166-172; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▆▇▇▇▇▇▇▇▇█████████████
wandb:          acc/val ▁▃▅▅▆▆▅▇▇▇▇█▇▇▇███▇██████
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▆▇▇▇▇▇▇▇▇█████████████
wandb:           f1/val ▁▃▅▅▆▆▄▇▇▇▇█▇▇████▇███▇██
wandb:        loss/test ▁
wandb:       loss/train █▄▄▄▄▃▃▃▂▃▂▃▃▂▂▂▂▃▂▂▂▁▂▂▂▁▂▁▁▂▁▂▁▂▁▁▁▁▂▁
wandb: loss/train_epoch █▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▄▃▂▂▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▃▅▅▆▆▆▇▇▇▇██████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▆▇▇▇▇▇▇▇▇▇▇███████████
wandb:    precision/val ▁▄▅▅▇▆▃▆▇▇▆█▆▇███▇▆███▇▇█
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▆▇▇▇▇▇▇▇▇█████████████
wandb:       recall/val ▁▃▅▅▆▆▆▇▇▇▇▇▇▇█▇██▇██████
wandb:     scheduler_lr ██████▇▇▇▇▇▆▆▆▅▅▅▄▄▃▃▂▂▁▁
wandb:        time/data ▂▂▂▂▂▃▃▃▂▃▂▂▂▃▂▂▂▂▂▂▁▂▁▂█▂▂▂▂▃▂▂▂▃▂▄▂▄▃▂
wandb:        time/iter ▃▃█▁▆▄▃▄▃▄▄▅▃▃▃▃▄▃▄▃▄▃▄▄▆▃▄▃▃▄▄▃▃▄▃█▃▃▄▃
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.91289
wandb:        acc/train 0.96023
wandb:          acc/val 0.8922
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.91763
wandb:         f1/train 0.9588
wandb:           f1/val 0.90166
wandb:        loss/test 0.30775
wandb:       loss/train 0.26502
wandb: loss/train_epoch 0.13126
wandb:         loss/val 0.3407
wandb:      max_acc/val 0.90329
wandb:   precision/test 0.91741
wandb:  precision/train 0.95659
wandb:    precision/val 0.9093
wandb:      recall/test 0.91944
wandb:     recall/train 0.96155
wandb:       recall/val 0.89858
wandb:     scheduler_lr 0.00053
wandb:        time/data 0.03083
wandb:        time/iter 0.05544
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/n7xtozhx
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_142003-n7xtozhx/logs
wandb: Agent Starting Run: rkgcfcr1 with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_142807-rkgcfcr1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/rkgcfcr1
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.001                                                  |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.514, loss: 1.577, precision: 0.568, recall: 0.497, f1: 0.507
new best found with: 0.722, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.722, loss: 1.024, precision: 0.767, recall: 0.703, f1: 0.719
train acc.: 0.765, loss: 0.811, precision: 0.779, recall: 0.757, f1: 0.765
new best found with: 0.758, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.758, loss: 0.751, precision: 0.820, recall: 0.756, f1: 0.770
train acc.: 0.825, loss: 0.586, precision: 0.835, recall: 0.821, f1: 0.827
new best found with: 0.820, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.820, loss: 0.567, precision: 0.854, recall: 0.818, f1: 0.826
train acc.: 0.857, loss: 0.477, precision: 0.862, recall: 0.852, f1: 0.856
new best found with: 0.834, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.834, loss: 0.525, precision: 0.864, recall: 0.836, f1: 0.844
train acc.: 0.872, loss: 0.424, precision: 0.876, recall: 0.866, f1: 0.870
new best found with: 0.851, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.851, loss: 0.495, precision: 0.863, recall: 0.855, f1: 0.855
train acc.: 0.890, loss: 0.367, precision: 0.893, recall: 0.884, f1: 0.887
new best found with: 0.862, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.862, loss: 0.445, precision: 0.885, recall: 0.863, f1: 0.871
train acc.: 0.893, loss: 0.347, precision: 0.897, recall: 0.887, f1: 0.891
val. acc.: 0.847, loss: 0.462, precision: 0.838, recall: 0.858, f1: 0.836
train acc.: 0.903, loss: 0.313, precision: 0.905, recall: 0.898, f1: 0.901
new best found with: 0.873, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.873, loss: 0.405, precision: 0.857, recall: 0.882, f1: 0.864
train acc.: 0.908, loss: 0.297, precision: 0.909, recall: 0.904, f1: 0.906
val. acc.: 0.870, loss: 0.385, precision: 0.887, recall: 0.872, f1: 0.877
train acc.: 0.912, loss: 0.277, precision: 0.913, recall: 0.908, f1: 0.910
val. acc.: 0.866, loss: 0.422, precision: 0.893, recall: 0.869, f1: 0.875
train acc.: 0.920, loss: 0.259, precision: 0.919, recall: 0.916, f1: 0.917
new best found with: 0.881, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.881, loss: 0.357, precision: 0.895, recall: 0.882, f1: 0.887
train acc.: 0.924, loss: 0.248, precision: 0.923, recall: 0.921, f1: 0.922
val. acc.: 0.880, loss: 0.387, precision: 0.892, recall: 0.889, f1: 0.887
train acc.: 0.926, loss: 0.236, precision: 0.923, recall: 0.923, f1: 0.923
val. acc.: 0.872, loss: 0.385, precision: 0.891, recall: 0.875, f1: 0.878
train acc.: 0.931, loss: 0.221, precision: 0.929, recall: 0.928, f1: 0.928
val. acc.: 0.870, loss: 0.421, precision: 0.889, recall: 0.870, f1: 0.876
train acc.: 0.931, loss: 0.217, precision: 0.928, recall: 0.931, f1: 0.929
new best found with: 0.892, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.892, loss: 0.352, precision: 0.877, recall: 0.901, f1: 0.882
train acc.: 0.933, loss: 0.212, precision: 0.930, recall: 0.932, f1: 0.931
val. acc.: 0.877, loss: 0.388, precision: 0.899, recall: 0.877, f1: 0.883
train acc.: 0.940, loss: 0.193, precision: 0.938, recall: 0.941, f1: 0.939
val. acc.: 0.873, loss: 0.385, precision: 0.889, recall: 0.882, f1: 0.882
train acc.: 0.940, loss: 0.191, precision: 0.935, recall: 0.940, f1: 0.937
new best found with: 0.895, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.895, loss: 0.336, precision: 0.877, recall: 0.903, f1: 0.883
train acc.: 0.945, loss: 0.180, precision: 0.940, recall: 0.946, f1: 0.943
new best found with: 0.897, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.897, loss: 0.352, precision: 0.909, recall: 0.897, f1: 0.902
train acc.: 0.943, loss: 0.177, precision: 0.938, recall: 0.943, f1: 0.940
new best found with: 0.902, save to logs/11_classes_full_dataset_sweep_M5/run_8/checkpoint
val. acc.: 0.902, loss: 0.337, precision: 0.908, recall: 0.909, f1: 0.908
train acc.: 0.947, loss: 0.168, precision: 0.941, recall: 0.949, f1: 0.944
val. acc.: 0.894, loss: 0.333, precision: 0.911, recall: 0.900, f1: 0.903
train acc.: 0.948, loss: 0.165, precision: 0.946, recall: 0.949, f1: 0.948
val. acc.: 0.901, loss: 0.320, precision: 0.881, recall: 0.909, f1: 0.889
train acc.: 0.951, loss: 0.155, precision: 0.946, recall: 0.952, f1: 0.949
val. acc.: 0.891, loss: 0.357, precision: 0.872, recall: 0.900, f1: 0.879
train acc.: 0.951, loss: 0.154, precision: 0.943, recall: 0.951, f1: 0.947
val. acc.: 0.882, loss: 0.367, precision: 0.885, recall: 0.892, f1: 0.883
train acc.: 0.952, loss: 0.150, precision: 0.947, recall: 0.954, f1: 0.950
val. acc.: 0.894, loss: 0.351, precision: 0.911, recall: 0.896, f1: 0.901
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.901, loss: 0.363, precision: 0.906, recall: 0.905, f1: 0.905
Epoch 24/50 ━━━━━━━━━━━━━╸                 48% 0:08:30 0:08:04 | best acc: 0.902wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▆▇▇▇▇▇▇▇██████████████
wandb:          acc/val ▁▂▅▅▆▆▆▇▇▇▇▇▇▇█▇▇██████▇█
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇▇▇▇▇██████████████
wandb:           f1/val ▁▃▅▆▆▇▅▆▇▇▇▇▇▇▇▇▇▇███▇▇▇█
wandb:        loss/test ▁
wandb:       loss/train ██▅▄▄▆▃▂▃▂▃▃▃▂▄▃▂▃▂▃▂▃▂▂▁▂▂▂▁▁▁▂▁▂▂▁▁▁▂▁
wandb: loss/train_epoch █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▃▃▃▂▂▂▂▂▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▂▅▅▆▆▆▇▇▇▇▇▇▇███████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▆▇▇▇▇▇▇▇██████████████
wandb:    precision/val ▁▄▅▆▆▇▄▅▇▇▇▇▇▇▆▇▇▆███▇▆▇█
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▆▇▇▇▇▇▇▇██████████████
wandb:       recall/val ▁▃▅▅▆▆▆▇▇▇▇▇▇▇█▇▇██████▇█
wandb:        time/data ▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        time/iter █▆▇▁▆▆▇▆█▆▇▆▆▆▆▇▆▆▇█▇█▆▆▇▇█▆▇▆▆▆▆▆▇▇▇▆▆█
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.90138
wandb:        acc/train 0.95222
wandb:          acc/val 0.89411
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.90491
wandb:         f1/train 0.94999
wandb:           f1/val 0.90085
wandb:        loss/test 0.36347
wandb:       loss/train 0.16515
wandb: loss/train_epoch 0.15033
wandb:         loss/val 0.3514
wandb:      max_acc/val 0.90176
wandb:   precision/test 0.90595
wandb:  precision/train 0.94697
wandb:    precision/val 0.91081
wandb:      recall/test 0.90527
wandb:     recall/train 0.95365
wandb:       recall/val 0.89618
wandb:        time/data 0.04506
wandb:        time/iter 0.06807
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/rkgcfcr1
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_142807-rkgcfcr1/logs
wandb: Agent Starting Run: khciwm9p with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': None, 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_143620-khciwm9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/khciwm9p
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.001                                                  |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.521, loss: 1.560, precision: 0.569, recall: 0.503, f1: 0.512
new best found with: 0.711, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.711, loss: 1.047, precision: 0.770, recall: 0.707, f1: 0.721
train acc.: 0.772, loss: 0.795, precision: 0.786, recall: 0.764, f1: 0.773
new best found with: 0.782, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.782, loss: 0.707, precision: 0.825, recall: 0.781, f1: 0.792
train acc.: 0.829, loss: 0.573, precision: 0.837, recall: 0.823, f1: 0.829
new best found with: 0.809, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.809, loss: 0.592, precision: 0.840, recall: 0.812, f1: 0.819
train acc.: 0.862, loss: 0.467, precision: 0.866, recall: 0.856, f1: 0.861
new best found with: 0.823, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.823, loss: 0.542, precision: 0.853, recall: 0.821, f1: 0.830
train acc.: 0.872, loss: 0.424, precision: 0.876, recall: 0.865, f1: 0.870
new best found with: 0.831, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.831, loss: 0.527, precision: 0.859, recall: 0.835, f1: 0.838
train acc.: 0.891, loss: 0.360, precision: 0.894, recall: 0.885, f1: 0.889
val. acc.: 0.829, loss: 0.533, precision: 0.860, recall: 0.833, f1: 0.838
train acc.: 0.894, loss: 0.340, precision: 0.897, recall: 0.888, f1: 0.892
new best found with: 0.854, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.854, loss: 0.436, precision: 0.853, recall: 0.863, f1: 0.849
train acc.: 0.904, loss: 0.307, precision: 0.905, recall: 0.900, f1: 0.902
new best found with: 0.854, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.854, loss: 0.441, precision: 0.841, recall: 0.864, f1: 0.839
train acc.: 0.909, loss: 0.295, precision: 0.910, recall: 0.904, f1: 0.907
new best found with: 0.869, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.869, loss: 0.376, precision: 0.889, recall: 0.869, f1: 0.876
train acc.: 0.913, loss: 0.275, precision: 0.915, recall: 0.908, f1: 0.911
val. acc.: 0.851, loss: 0.463, precision: 0.887, recall: 0.853, f1: 0.861
train acc.: 0.921, loss: 0.254, precision: 0.921, recall: 0.918, f1: 0.919
new best found with: 0.871, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.871, loss: 0.381, precision: 0.856, recall: 0.879, f1: 0.858
train acc.: 0.927, loss: 0.236, precision: 0.926, recall: 0.927, f1: 0.926
new best found with: 0.891, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.891, loss: 0.358, precision: 0.871, recall: 0.900, f1: 0.877
train acc.: 0.929, loss: 0.227, precision: 0.926, recall: 0.928, f1: 0.927
val. acc.: 0.872, loss: 0.390, precision: 0.896, recall: 0.871, f1: 0.879
train acc.: 0.929, loss: 0.221, precision: 0.927, recall: 0.928, f1: 0.927
val. acc.: 0.880, loss: 0.379, precision: 0.899, recall: 0.879, f1: 0.886
train acc.: 0.933, loss: 0.216, precision: 0.929, recall: 0.933, f1: 0.931
val. acc.: 0.876, loss: 0.373, precision: 0.894, recall: 0.878, f1: 0.883
train acc.: 0.934, loss: 0.206, precision: 0.931, recall: 0.934, f1: 0.932
val. acc.: 0.886, loss: 0.348, precision: 0.898, recall: 0.893, f1: 0.894
train acc.: 0.941, loss: 0.193, precision: 0.938, recall: 0.942, f1: 0.940
new best found with: 0.894, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.894, loss: 0.340, precision: 0.906, recall: 0.894, f1: 0.899
train acc.: 0.935, loss: 0.201, precision: 0.930, recall: 0.934, f1: 0.932
new best found with: 0.898, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.898, loss: 0.323, precision: 0.910, recall: 0.906, f1: 0.906
train acc.: 0.944, loss: 0.178, precision: 0.937, recall: 0.944, f1: 0.940
val. acc.: 0.893, loss: 0.332, precision: 0.901, recall: 0.901, f1: 0.901
train acc.: 0.943, loss: 0.175, precision: 0.940, recall: 0.944, f1: 0.942
val. acc.: 0.881, loss: 0.377, precision: 0.861, recall: 0.890, f1: 0.862
train acc.: 0.945, loss: 0.170, precision: 0.940, recall: 0.946, f1: 0.943
val. acc.: 0.894, loss: 0.361, precision: 0.900, recall: 0.902, f1: 0.900
train acc.: 0.948, loss: 0.163, precision: 0.945, recall: 0.948, f1: 0.946
new best found with: 0.899, save to logs/11_classes_full_dataset_sweep_M5/run_9/checkpoint
val. acc.: 0.899, loss: 0.338, precision: 0.889, recall: 0.907, f1: 0.893
train acc.: 0.951, loss: 0.159, precision: 0.946, recall: 0.953, f1: 0.949
val. acc.: 0.880, loss: 0.383, precision: 0.900, recall: 0.885, f1: 0.888
train acc.: 0.952, loss: 0.152, precision: 0.946, recall: 0.953, f1: 0.949
val. acc.: 0.888, loss: 0.360, precision: 0.869, recall: 0.897, f1: 0.875
train acc.: 0.954, loss: 0.148, precision: 0.950, recall: 0.954, f1: 0.952
val. acc.: 0.894, loss: 0.344, precision: 0.909, recall: 0.903, f1: 0.904
train acc.: 0.954, loss: 0.145, precision: 0.952, recall: 0.953, f1: 0.952
val. acc.: 0.894, loss: 0.349, precision: 0.908, recall: 0.902, f1: 0.903
train acc.: 0.953, loss: 0.148, precision: 0.950, recall: 0.954, f1: 0.951
val. acc.: 0.881, loss: 0.388, precision: 0.860, recall: 0.890, f1: 0.855
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.907, loss: 0.326, precision: 0.892, recall: 0.914, f1: 0.897
Epoch 26/50 ━━━━━━━━━━━━━━━                52% 0:07:43 0:08:46 | best acc: 0.899wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇▇▇▇▇████████████████
wandb:          acc/val ▁▄▅▅▅▅▆▆▇▆▇█▇▇▇████▇██▇███▇
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇▇▇▇▇████████████████
wandb:           f1/val ▁▄▅▅▅▅▆▅▇▆▆▇▇▇▇████▆██▇▇██▆
wandb:        loss/test ▁
wandb:       loss/train █▇▃▂▃▃▂▂▂▃▂▃▂▂▂▂▃▁▂▂▂▂▁▁▂▂▂▁▁▁▂▁▁▂▂▂▁▁▁▂
wandb: loss/train_epoch █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▄▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▂▁▁▂▁▁▁▂
wandb:      max_acc/val ▁▄▅▅▅▅▆▆▇▇▇████████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▆▇▇▇▇▇▇▇████████████████
wandb:    precision/val ▁▄▅▅▅▆▅▅▇▇▅▆▇▇▇▇███▆█▇█▆██▆
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▆▇▇▇▇▇▇▇████████████████
wandb:       recall/val ▁▄▅▅▅▅▆▆▇▆▇█▇▇▇████▇██▇███▇
wandb:        time/data █▂▂▂▂▂▂▂▂▁▂▃▂▂▂▁▁▁▁▁▁▂▁▁▂▂▃▂▂▅▂▄▁▂▂▂▃▁▂▂
wandb:        time/iter ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.90675
wandb:        acc/train 0.95344
wandb:          acc/val 0.88112
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.89697
wandb:         f1/train 0.95146
wandb:           f1/val 0.85549
wandb:        loss/test 0.32602
wandb:       loss/train 0.1093
wandb: loss/train_epoch 0.14775
wandb:         loss/val 0.38836
wandb:      max_acc/val 0.8987
wandb:   precision/test 0.89156
wandb:  precision/train 0.94973
wandb:    precision/val 0.86029
wandb:      recall/test 0.91388
wandb:     recall/train 0.95362
wandb:       recall/val 0.89043
wandb:        time/data 0.04209
wandb:        time/iter 0.0649
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/khciwm9p
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_143620-khciwm9p/logs
wandb: Agent Starting Run: w8bsje24 with config:
wandb: 	sweep: {'training': {'lr': 0.003, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_144513-w8bsje24
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/w8bsje24
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.003                                                  |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.584, loss: 1.298, precision: 0.618, recall: 0.572, f1: 0.582
new best found with: 0.720, save to logs/11_classes_full_dataset_sweep_M5/run_10/checkpoint
val. acc.: 0.720, loss: 0.870, precision: 0.771, recall: 0.724, f1: 0.733
train acc.: 0.806, loss: 0.631, precision: 0.813, recall: 0.802, f1: 0.807
new best found with: 0.811, save to logs/11_classes_full_dataset_sweep_M5/run_10/checkpoint
val. acc.: 0.811, loss: 0.606, precision: 0.852, recall: 0.814, f1: 0.824
train acc.: 0.854, loss: 0.470, precision: 0.856, recall: 0.852, f1: 0.854
val. acc.: 0.803, loss: 0.611, precision: 0.844, recall: 0.811, f1: 0.816
train acc.: 0.878, loss: 0.384, precision: 0.880, recall: 0.877, f1: 0.878
new best found with: 0.857, save to logs/11_classes_full_dataset_sweep_M5/run_10/checkpoint
val. acc.: 0.857, loss: 0.441, precision: 0.841, recall: 0.864, f1: 0.842
train acc.: 0.888, loss: 0.352, precision: 0.888, recall: 0.886, f1: 0.887
new best found with: 0.860, save to logs/11_classes_full_dataset_sweep_M5/run_10/checkpoint
val. acc.: 0.860, loss: 0.429, precision: 0.875, recall: 0.866, f1: 0.868
train acc.: 0.903, loss: 0.308, precision: 0.899, recall: 0.900, f1: 0.900
new best found with: 0.863, save to logs/11_classes_full_dataset_sweep_M5/run_10/checkpoint
val. acc.: 0.863, loss: 0.435, precision: 0.880, recall: 0.866, f1: 0.870
train acc.: 0.910, loss: 0.284, precision: 0.908, recall: 0.909, f1: 0.908
val. acc.: 0.851, loss: 0.462, precision: 0.836, recall: 0.863, f1: 0.826
train acc.: 0.920, loss: 0.251, precision: 0.915, recall: 0.922, f1: 0.918
new best found with: 0.875, save to logs/11_classes_full_dataset_sweep_M5/run_10/checkpoint
val. acc.: 0.875, loss: 0.384, precision: 0.888, recall: 0.878, f1: 0.881
train acc.: 0.923, loss: 0.243, precision: 0.920, recall: 0.924, f1: 0.922
new best found with: 0.891, save to logs/11_classes_full_dataset_sweep_M5/run_10/checkpoint
val. acc.: 0.891, loss: 0.341, precision: 0.900, recall: 0.897, f1: 0.898
train acc.: 0.925, loss: 0.235, precision: 0.918, recall: 0.925, f1: 0.921
val. acc.: 0.813, loss: 0.591, precision: 0.858, recall: 0.821, f1: 0.827
train acc.: 0.933, loss: 0.211, precision: 0.927, recall: 0.933, f1: 0.930
val. acc.: 0.846, loss: 0.480, precision: 0.884, recall: 0.850, f1: 0.852
train acc.: 0.937, loss: 0.196, precision: 0.933, recall: 0.939, f1: 0.935
val. acc.: 0.884, loss: 0.373, precision: 0.899, recall: 0.881, f1: 0.886
train acc.: 0.940, loss: 0.187, precision: 0.933, recall: 0.942, f1: 0.937
val. acc.: 0.867, loss: 0.435, precision: 0.892, recall: 0.868, f1: 0.874
train acc.: 0.939, loss: 0.184, precision: 0.935, recall: 0.940, f1: 0.937
val. acc.: 0.890, loss: 0.350, precision: 0.902, recall: 0.888, f1: 0.893
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.895, loss: 0.361, precision: 0.902, recall: 0.902, f1: 0.900
Epoch 13/50 ━━━━━━━╸                       26% 0:11:51 0:04:36 | best acc: 0.891wandb: uploading history steps 4065-4130, summary, console lines 139-145
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇███████
wandb:          acc/val ▁▅▄▇▇▇▆▇█▅▆█▇█
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇███████
wandb:           f1/val ▁▅▅▆▇▇▅▇█▅▆█▇█
wandb:        loss/test ▁
wandb:       loss/train █▅▄▄▄▃▂▃▃▂▃▂▃▂▃▃▃▂▂▂▁▁▂▂▂▃▂▁▂▂▂▂▃▃▁▂▂▁▂▁
wandb: loss/train_epoch █▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:         loss/val █▅▅▂▂▂▃▂▁▄▃▁▂▁
wandb:      max_acc/val ▁▅▅▇▇▇▇▇██████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▇▇▇▇███████
wandb:    precision/val ▁▅▅▅▇▇▄▇█▆▇███
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇▇███████
wandb:       recall/val ▁▅▅▇▇▇▇▇█▅▆▇▇█
wandb:     scheduler_lr ████▇▇▆▆▅▅▄▃▂▁
wandb:        time/data ▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        time/iter ▆▆▅▁▆▆█▆▅▇▇▆▆▆▆▅▆▆▆▆▆▇▆▆▇▆▇▆█▆▇▅▆█▆▆▅▅▆▆
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.89524
wandb:        acc/train 0.93923
wandb:          acc/val 0.88953
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.90027
wandb:         f1/train 0.93726
wandb:           f1/val 0.89343
wandb:        loss/test 0.36127
wandb:       loss/train 0.06638
wandb: loss/train_epoch 0.1841
wandb:         loss/val 0.35012
wandb:      max_acc/val 0.89067
wandb:   precision/test 0.90232
wandb:  precision/train 0.93475
wandb:    precision/val 0.90156
wandb:      recall/test 0.90164
wandb:     recall/train 0.94046
wandb:       recall/val 0.88838
wandb:     scheduler_lr 0.00253
wandb:        time/data 0.04139
wandb:        time/iter 0.06551
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/w8bsje24
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_144513-w8bsje24/logs
wandb: Agent Starting Run: etf46h95 with config:
wandb: 	sweep: {'training': {'lr': 0.003, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_144959-etf46h95
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/etf46h95
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.003                                                  |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.587, loss: 1.292, precision: 0.616, recall: 0.575, f1: 0.585
new best found with: 0.576, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.576, loss: 1.262, precision: 0.749, recall: 0.603, f1: 0.611
train acc.: 0.808, loss: 0.620, precision: 0.815, recall: 0.803, f1: 0.808
new best found with: 0.762, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.762, loss: 0.718, precision: 0.851, recall: 0.771, f1: 0.779
train acc.: 0.859, loss: 0.462, precision: 0.861, recall: 0.854, f1: 0.857
new best found with: 0.830, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.830, loss: 0.520, precision: 0.861, recall: 0.832, f1: 0.840
train acc.: 0.879, loss: 0.382, precision: 0.883, recall: 0.875, f1: 0.879
new best found with: 0.833, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.833, loss: 0.506, precision: 0.871, recall: 0.834, f1: 0.844
train acc.: 0.891, loss: 0.348, precision: 0.892, recall: 0.889, f1: 0.891
new best found with: 0.847, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.847, loss: 0.464, precision: 0.873, recall: 0.844, f1: 0.853
train acc.: 0.903, loss: 0.304, precision: 0.898, recall: 0.901, f1: 0.899
val. acc.: 0.795, loss: 0.651, precision: 0.856, recall: 0.804, f1: 0.804
train acc.: 0.909, loss: 0.280, precision: 0.905, recall: 0.908, f1: 0.907
val. acc.: 0.847, loss: 0.452, precision: 0.828, recall: 0.858, f1: 0.826
train acc.: 0.916, loss: 0.259, precision: 0.912, recall: 0.919, f1: 0.915
new best found with: 0.885, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.885, loss: 0.361, precision: 0.898, recall: 0.894, f1: 0.895
train acc.: 0.920, loss: 0.248, precision: 0.913, recall: 0.920, f1: 0.916
new best found with: 0.888, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.888, loss: 0.354, precision: 0.902, recall: 0.887, f1: 0.892
train acc.: 0.926, loss: 0.230, precision: 0.920, recall: 0.927, f1: 0.923
val. acc.: 0.857, loss: 0.440, precision: 0.894, recall: 0.858, f1: 0.866
train acc.: 0.932, loss: 0.215, precision: 0.923, recall: 0.934, f1: 0.928
val. acc.: 0.887, loss: 0.346, precision: 0.868, recall: 0.896, f1: 0.875
train acc.: 0.934, loss: 0.199, precision: 0.928, recall: 0.937, f1: 0.932
val. acc.: 0.887, loss: 0.386, precision: 0.879, recall: 0.896, f1: 0.881
train acc.: 0.938, loss: 0.192, precision: 0.930, recall: 0.940, f1: 0.934
val. acc.: 0.814, loss: 0.713, precision: 0.828, recall: 0.829, f1: 0.809
train acc.: 0.939, loss: 0.186, precision: 0.932, recall: 0.941, f1: 0.936
new best found with: 0.892, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.892, loss: 0.361, precision: 0.909, recall: 0.892, f1: 0.897
train acc.: 0.940, loss: 0.181, precision: 0.935, recall: 0.942, f1: 0.938
val. acc.: 0.885, loss: 0.378, precision: 0.893, recall: 0.894, f1: 0.891
train acc.: 0.948, loss: 0.163, precision: 0.943, recall: 0.950, f1: 0.946
val. acc.: 0.888, loss: 0.373, precision: 0.871, recall: 0.897, f1: 0.877
train acc.: 0.948, loss: 0.157, precision: 0.944, recall: 0.949, f1: 0.946
new best found with: 0.900, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.900, loss: 0.352, precision: 0.906, recall: 0.908, f1: 0.905
train acc.: 0.950, loss: 0.151, precision: 0.946, recall: 0.952, f1: 0.948
val. acc.: 0.881, loss: 0.368, precision: 0.861, recall: 0.891, f1: 0.864
train acc.: 0.954, loss: 0.140, precision: 0.950, recall: 0.956, f1: 0.953
val. acc.: 0.896, loss: 0.362, precision: 0.900, recall: 0.900, f1: 0.899
train acc.: 0.954, loss: 0.142, precision: 0.948, recall: 0.956, f1: 0.952
val. acc.: 0.890, loss: 0.359, precision: 0.904, recall: 0.890, f1: 0.895
train acc.: 0.958, loss: 0.129, precision: 0.955, recall: 0.960, f1: 0.957
new best found with: 0.913, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.913, loss: 0.321, precision: 0.916, recall: 0.920, f1: 0.917
train acc.: 0.959, loss: 0.125, precision: 0.956, recall: 0.960, f1: 0.958
val. acc.: 0.897, loss: 0.356, precision: 0.906, recall: 0.905, f1: 0.905
train acc.: 0.964, loss: 0.117, precision: 0.959, recall: 0.966, f1: 0.962
val. acc.: 0.896, loss: 0.346, precision: 0.901, recall: 0.904, f1: 0.900
train acc.: 0.965, loss: 0.111, precision: 0.962, recall: 0.966, f1: 0.964
val. acc.: 0.890, loss: 0.386, precision: 0.867, recall: 0.898, f1: 0.874
train acc.: 0.964, loss: 0.111, precision: 0.962, recall: 0.965, f1: 0.963
new best found with: 0.914, save to logs/11_classes_full_dataset_sweep_M5/run_11/checkpoint
val. acc.: 0.914, loss: 0.335, precision: 0.915, recall: 0.921, f1: 0.917
train acc.: 0.968, loss: 0.102, precision: 0.965, recall: 0.969, f1: 0.967
val. acc.: 0.913, loss: 0.324, precision: 0.924, recall: 0.920, f1: 0.921
train acc.: 0.971, loss: 0.092, precision: 0.968, recall: 0.972, f1: 0.970
val. acc.: 0.904, loss: 0.339, precision: 0.909, recall: 0.912, f1: 0.909
train acc.: 0.971, loss: 0.090, precision: 0.969, recall: 0.972, f1: 0.970
val. acc.: 0.899, loss: 0.376, precision: 0.914, recall: 0.899, f1: 0.904
train acc.: 0.970, loss: 0.094, precision: 0.966, recall: 0.971, f1: 0.968
val. acc.: 0.905, loss: 0.342, precision: 0.912, recall: 0.912, f1: 0.911
train acc.: 0.972, loss: 0.089, precision: 0.971, recall: 0.973, f1: 0.972
val. acc.: 0.909, loss: 0.333, precision: 0.920, recall: 0.916, f1: 0.917
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.915, loss: 0.342, precision: 0.918, recall: 0.919, f1: 0.918
Epoch 29/50 ━━━━━━━━━━━━━━━━╸              58% 0:06:45 0:09:48 | best acc: 0.914wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████
wandb:          acc/val ▁▅▆▆▇▆▇▇▇▇▇▇▆█▇▇█▇█▇███▇██████
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████
wandb:           f1/val ▁▅▆▆▆▅▆▇▇▇▇▇▅▇▇▇█▇█▇███▇██████
wandb:        loss/test ▁
wandb:       loss/train █▇▄▃▂▁▂▂▂▁▂▂▂▂▁▂▁▂▁▂▁▁▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁
wandb: loss/train_epoch █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▄▂▂▂▃▂▁▁▂▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▅▆▆▇▇▇▇▇▇▇▇▇█████████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████
wandb:    precision/val ▁▅▅▆▆▅▄▇▇▇▆▆▄▇▇▆▇▅▇▇█▇▇▆██▇███
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████
wandb:       recall/val ▁▅▆▆▆▅▇▇▇▇▇▇▆▇▇▇█▇█▇██████████
wandb:     scheduler_lr ███████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁
wandb:        time/data ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▂▁▁▁▁
wandb:        time/iter ▄▄▃▄▃▄█▃▃▃▃▄▃▃▃▄▃▄▃▃▁█▃▄▃▄▁▄▃▃▃▄▄▃▃▄▃▇▃▃
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.91481
wandb:        acc/train 0.97184
wandb:          acc/val 0.90902
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.91773
wandb:         f1/train 0.97195
wandb:           f1/val 0.91684
wandb:        loss/test 0.34206
wandb:       loss/train 0.02187
wandb: loss/train_epoch 0.08861
wandb:         loss/val 0.33343
wandb:      max_acc/val 0.91399
wandb:   precision/test 0.91841
wandb:  precision/train 0.97073
wandb:    precision/val 0.91996
wandb:      recall/test 0.91919
wandb:     recall/train 0.97348
wandb:       recall/val 0.91578
wandb:     scheduler_lr 0.00113
wandb:        time/data 0.04914
wandb:        time/iter 0.07293
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/etf46h95
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_144959-etf46h95/logs
wandb: Agent Starting Run: hzkh1bcn with config:
wandb: 	sweep: {'training': {'lr': 0.003, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_145956-hzkh1bcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/hzkh1bcn
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.003                                                  |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.584, loss: 1.298, precision: 0.618, recall: 0.572, f1: 0.582
new best found with: 0.720, save to logs/11_classes_full_dataset_sweep_M5/run_12/checkpoint
val. acc.: 0.720, loss: 0.870, precision: 0.771, recall: 0.724, f1: 0.733
train acc.: 0.804, loss: 0.635, precision: 0.811, recall: 0.799, f1: 0.804
new best found with: 0.806, save to logs/11_classes_full_dataset_sweep_M5/run_12/checkpoint
val. acc.: 0.806, loss: 0.604, precision: 0.818, recall: 0.814, f1: 0.808
train acc.: 0.857, loss: 0.463, precision: 0.860, recall: 0.854, f1: 0.856
new best found with: 0.835, save to logs/11_classes_full_dataset_sweep_M5/run_12/checkpoint
val. acc.: 0.835, loss: 0.498, precision: 0.864, recall: 0.838, f1: 0.844
train acc.: 0.878, loss: 0.389, precision: 0.880, recall: 0.876, f1: 0.878
val. acc.: 0.818, loss: 0.568, precision: 0.869, recall: 0.824, f1: 0.836
train acc.: 0.887, loss: 0.358, precision: 0.888, recall: 0.884, f1: 0.886
new best found with: 0.878, save to logs/11_classes_full_dataset_sweep_M5/run_12/checkpoint
val. acc.: 0.878, loss: 0.402, precision: 0.859, recall: 0.886, f1: 0.867
train acc.: 0.901, loss: 0.313, precision: 0.898, recall: 0.899, f1: 0.898
val. acc.: 0.804, loss: 0.609, precision: 0.853, recall: 0.812, f1: 0.814
train acc.: 0.909, loss: 0.283, precision: 0.908, recall: 0.908, f1: 0.908
val. acc.: 0.842, loss: 0.476, precision: 0.833, recall: 0.855, f1: 0.819
train acc.: 0.917, loss: 0.261, precision: 0.909, recall: 0.919, f1: 0.914
val. acc.: 0.762, loss: 0.753, precision: 0.795, recall: 0.782, f1: 0.736
train acc.: 0.919, loss: 0.255, precision: 0.914, recall: 0.921, f1: 0.917
new best found with: 0.885, save to logs/11_classes_full_dataset_sweep_M5/run_12/checkpoint
val. acc.: 0.885, loss: 0.340, precision: 0.900, recall: 0.886, f1: 0.891
train acc.: 0.924, loss: 0.243, precision: 0.918, recall: 0.924, f1: 0.921
val. acc.: 0.867, loss: 0.403, precision: 0.889, recall: 0.869, f1: 0.875
train acc.: 0.928, loss: 0.223, precision: 0.924, recall: 0.931, f1: 0.927
new best found with: 0.896, save to logs/11_classes_full_dataset_sweep_M5/run_12/checkpoint
val. acc.: 0.896, loss: 0.334, precision: 0.905, recall: 0.904, f1: 0.903
train acc.: 0.935, loss: 0.201, precision: 0.932, recall: 0.938, f1: 0.935
val. acc.: 0.881, loss: 0.395, precision: 0.857, recall: 0.890, f1: 0.860
train acc.: 0.935, loss: 0.199, precision: 0.930, recall: 0.937, f1: 0.933
val. acc.: 0.871, loss: 0.409, precision: 0.849, recall: 0.881, f1: 0.852
train acc.: 0.935, loss: 0.198, precision: 0.927, recall: 0.936, f1: 0.931
val. acc.: 0.875, loss: 0.398, precision: 0.866, recall: 0.878, f1: 0.865
train acc.: 0.937, loss: 0.189, precision: 0.934, recall: 0.938, f1: 0.936
val. acc.: 0.881, loss: 0.393, precision: 0.870, recall: 0.890, f1: 0.872
train acc.: 0.944, loss: 0.175, precision: 0.940, recall: 0.945, f1: 0.943
val. acc.: 0.876, loss: 0.407, precision: 0.853, recall: 0.887, f1: 0.857
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.903, loss: 0.348, precision: 0.901, recall: 0.910, f1: 0.905
Epoch 15/50 ━━━━━━━━╸                      30% 0:11:03 0:05:13 | best acc: 0.896wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇▇████████
wandb:          acc/val ▁▄▆▅▇▄▆▃█▇█▇▇▇▇▇
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇▇▇███████
wandb:           f1/val ▁▄▆▅▆▄▅▁▇▇█▆▆▆▇▆
wandb:        loss/test ▁
wandb:       loss/train █▆▅▄▅▃▂▂▂▃▂▂▂▂▂▃▂▃▂▂▂▁▁▂▂▂▂▁▁▂▁▂▁▂▂▂▁▂▁▂
wandb: loss/train_epoch █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▃▄▂▅▃▆▁▂▁▂▂▂▂▂
wandb:      max_acc/val ▁▄▆▆▇▇▇▇████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▇▇▇▇▇▇███████
wandb:    precision/val ▁▃▆▆▆▅▄▂█▇█▅▅▆▆▅
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇▇█████████
wandb:       recall/val ▁▄▅▅▇▄▆▃▇▇█▇▇▇▇▇
wandb:        time/data ▁▆▆▆▇▆█▇▇▇▆▇▆▆▆▇▇▆▆▆▅▆▇█▆▆▆▇▆▆▁▆▆▇▆▆▇▆▆▆
wandb:        time/iter ▁▁▁▁▂▂▁█▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.9033
wandb:        acc/train 0.94421
wandb:          acc/val 0.87615
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.90457
wandb:         f1/train 0.94259
wandb:           f1/val 0.85724
wandb:        loss/test 0.34776
wandb:       loss/train 0.30689
wandb: loss/train_epoch 0.17538
wandb:         loss/val 0.40688
wandb:      max_acc/val 0.89602
wandb:   precision/test 0.90133
wandb:  precision/train 0.94029
wandb:    precision/val 0.85346
wandb:      recall/test 0.91043
wandb:     recall/train 0.94531
wandb:       recall/val 0.88677
wandb:        time/data 0.05235
wandb:        time/iter 0.07432
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/hzkh1bcn
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_145956-hzkh1bcn/logs
wandb: Agent Starting Run: b9gsraoa with config:
wandb: 	sweep: {'training': {'lr': 0.003, 'scheduler': None, 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_150518-b9gsraoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/b9gsraoa
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.003                                                  |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.587, loss: 1.292, precision: 0.616, recall: 0.575, f1: 0.585
new best found with: 0.576, save to logs/11_classes_full_dataset_sweep_M5/run_13/checkpoint
val. acc.: 0.576, loss: 1.262, precision: 0.749, recall: 0.603, f1: 0.611
train acc.: 0.812, loss: 0.616, precision: 0.819, recall: 0.807, f1: 0.812
new best found with: 0.807, save to logs/11_classes_full_dataset_sweep_M5/run_13/checkpoint
val. acc.: 0.807, loss: 0.580, precision: 0.844, recall: 0.812, f1: 0.821
train acc.: 0.857, loss: 0.456, precision: 0.861, recall: 0.854, f1: 0.857
new best found with: 0.811, save to logs/11_classes_full_dataset_sweep_M5/run_13/checkpoint
val. acc.: 0.811, loss: 0.575, precision: 0.847, recall: 0.816, f1: 0.817
train acc.: 0.878, loss: 0.385, precision: 0.880, recall: 0.875, f1: 0.877
new best found with: 0.836, save to logs/11_classes_full_dataset_sweep_M5/run_13/checkpoint
val. acc.: 0.836, loss: 0.506, precision: 0.875, recall: 0.839, f1: 0.849
train acc.: 0.885, loss: 0.358, precision: 0.883, recall: 0.882, f1: 0.882
new best found with: 0.849, save to logs/11_classes_full_dataset_sweep_M5/run_13/checkpoint
val. acc.: 0.849, loss: 0.460, precision: 0.875, recall: 0.849, f1: 0.856
train acc.: 0.903, loss: 0.300, precision: 0.898, recall: 0.901, f1: 0.899
val. acc.: 0.849, loss: 0.468, precision: 0.873, recall: 0.851, f1: 0.858
train acc.: 0.910, loss: 0.286, precision: 0.904, recall: 0.910, f1: 0.907
new best found with: 0.883, save to logs/11_classes_full_dataset_sweep_M5/run_13/checkpoint
val. acc.: 0.883, loss: 0.381, precision: 0.894, recall: 0.890, f1: 0.890
train acc.: 0.916, loss: 0.263, precision: 0.910, recall: 0.918, f1: 0.914
val. acc.: 0.833, loss: 0.547, precision: 0.829, recall: 0.846, f1: 0.808
train acc.: 0.920, loss: 0.249, precision: 0.912, recall: 0.921, f1: 0.916
val. acc.: 0.881, loss: 0.372, precision: 0.898, recall: 0.880, f1: 0.886
train acc.: 0.923, loss: 0.235, precision: 0.917, recall: 0.923, f1: 0.920
val. acc.: 0.861, loss: 0.445, precision: 0.884, recall: 0.864, f1: 0.868
train acc.: 0.930, loss: 0.218, precision: 0.923, recall: 0.932, f1: 0.927
val. acc.: 0.872, loss: 0.392, precision: 0.892, recall: 0.872, f1: 0.878
train acc.: 0.935, loss: 0.203, precision: 0.930, recall: 0.936, f1: 0.933
val. acc.: 0.871, loss: 0.462, precision: 0.897, recall: 0.871, f1: 0.877
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.891, loss: 0.362, precision: 0.897, recall: 0.895, f1: 0.895
Epoch 11/50 ━━━━━━                         22% 0:12:19 0:03:56 | best acc: 0.883wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▆▆▇▇▇▇█████
wandb:          acc/val ▁▆▆▇▇▇█▇████
wandb:          f1/test ▁
wandb:         f1/train ▁▆▆▇▇▇▇█████
wandb:           f1/val ▁▆▆▇▇▇█▆█▇██
wandb:        loss/test ▁
wandb:       loss/train █▄▂▃▃▂▂▂▃▃▂▃▂▂▂▂▃▂▁▂▂▁▁▂▂▁▁▂▂▁▁▃▂▂▂▁▂▁▁▁
wandb: loss/train_epoch █▄▃▂▂▂▂▁▁▁▁▁
wandb:         loss/val █▃▃▂▂▂▁▂▁▂▁▂
wandb:      max_acc/val ▁▆▆▇▇▇██████
wandb:   precision/test ▁
wandb:  precision/train ▁▆▆▇▇▇▇█████
wandb:    precision/val ▁▅▆▇▇▇█▅█▇██
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇▇█████
wandb:       recall/val ▁▆▆▇▇▇█▇█▇██
wandb:        time/data ▂▂▂▁▂▁▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▂▂▂▁█▂▁▁▂▂▂▁▂▃▁▂▁▂▁
wandb:        time/iter ▄▄▄▄▁▄▄▅▄▅▄▄▄▅▄▄▄▄▄▄▅▄▅▄▄█▄▄▄▄▅▄▇▄▄▄▄▄▄▄
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.89064
wandb:        acc/train 0.93472
wandb:          acc/val 0.87118
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.89497
wandb:         f1/train 0.9326
wandb:           f1/val 0.87699
wandb:        loss/test 0.36222
wandb:       loss/train 0.15388
wandb: loss/train_epoch 0.20327
wandb:         loss/val 0.46207
wandb:      max_acc/val 0.88265
wandb:   precision/test 0.89661
wandb:  precision/train 0.92959
wandb:    precision/val 0.89665
wandb:      recall/test 0.89506
wandb:     recall/train 0.93629
wandb:       recall/val 0.87112
wandb:        time/data 0.04403
wandb:        time/iter 0.09107
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/b9gsraoa
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_150518-b9gsraoa/logs
wandb: Agent Starting Run: 42q1gjbe with config:
wandb: 	sweep: {'training': {'lr': 0.01, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_150921-42q1gjbe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/42q1gjbe
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.01                                                   |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.566, loss: 1.272, precision: 0.595, recall: 0.561, f1: 0.571
new best found with: 0.444, save to logs/11_classes_full_dataset_sweep_M5/run_14/checkpoint
val. acc.: 0.444, loss: 1.772, precision: 0.703, recall: 0.472, f1: 0.458
train acc.: 0.800, loss: 0.628, precision: 0.807, recall: 0.802, f1: 0.804
new best found with: 0.727, save to logs/11_classes_full_dataset_sweep_M5/run_14/checkpoint
val. acc.: 0.727, loss: 0.829, precision: 0.792, recall: 0.734, f1: 0.740
train acc.: 0.853, loss: 0.457, precision: 0.848, recall: 0.854, f1: 0.850
new best found with: 0.812, save to logs/11_classes_full_dataset_sweep_M5/run_14/checkpoint
val. acc.: 0.812, loss: 0.555, precision: 0.851, recall: 0.818, f1: 0.826
train acc.: 0.876, loss: 0.387, precision: 0.871, recall: 0.880, f1: 0.875
val. acc.: 0.788, loss: 0.645, precision: 0.851, recall: 0.792, f1: 0.803
train acc.: 0.880, loss: 0.375, precision: 0.872, recall: 0.882, f1: 0.876
new best found with: 0.869, save to logs/11_classes_full_dataset_sweep_M5/run_14/checkpoint
val. acc.: 0.869, loss: 0.403, precision: 0.858, recall: 0.877, f1: 0.860
train acc.: 0.896, loss: 0.314, precision: 0.890, recall: 0.899, f1: 0.894
new best found with: 0.887, save to logs/11_classes_full_dataset_sweep_M5/run_14/checkpoint
val. acc.: 0.887, loss: 0.374, precision: 0.865, recall: 0.896, f1: 0.871
train acc.: 0.908, loss: 0.293, precision: 0.902, recall: 0.912, f1: 0.906
val. acc.: 0.876, loss: 0.391, precision: 0.894, recall: 0.877, f1: 0.882
train acc.: 0.912, loss: 0.268, precision: 0.906, recall: 0.917, f1: 0.911
val. acc.: 0.867, loss: 0.426, precision: 0.889, recall: 0.869, f1: 0.874
train acc.: 0.917, loss: 0.257, precision: 0.913, recall: 0.922, f1: 0.916
val. acc.: 0.886, loss: 0.355, precision: 0.870, recall: 0.895, f1: 0.877
train acc.: 0.918, loss: 0.247, precision: 0.914, recall: 0.921, f1: 0.917
val. acc.: 0.863, loss: 0.446, precision: 0.858, recall: 0.872, f1: 0.857
train acc.: 0.923, loss: 0.231, precision: 0.919, recall: 0.928, f1: 0.923
new best found with: 0.888, save to logs/11_classes_full_dataset_sweep_M5/run_14/checkpoint
val. acc.: 0.888, loss: 0.365, precision: 0.862, recall: 0.897, f1: 0.868
train acc.: 0.930, loss: 0.211, precision: 0.927, recall: 0.933, f1: 0.930
val. acc.: 0.875, loss: 0.431, precision: 0.899, recall: 0.876, f1: 0.882
train acc.: 0.932, loss: 0.204, precision: 0.929, recall: 0.935, f1: 0.932
val. acc.: 0.870, loss: 0.435, precision: 0.886, recall: 0.880, f1: 0.880
train acc.: 0.932, loss: 0.205, precision: 0.930, recall: 0.935, f1: 0.932
new best found with: 0.893, save to logs/11_classes_full_dataset_sweep_M5/run_14/checkpoint
val. acc.: 0.893, loss: 0.373, precision: 0.870, recall: 0.901, f1: 0.874
train acc.: 0.937, loss: 0.188, precision: 0.935, recall: 0.939, f1: 0.937
val. acc.: 0.883, loss: 0.399, precision: 0.905, recall: 0.884, f1: 0.889
train acc.: 0.941, loss: 0.179, precision: 0.940, recall: 0.945, f1: 0.942
val. acc.: 0.883, loss: 0.408, precision: 0.901, recall: 0.884, f1: 0.889
train acc.: 0.941, loss: 0.177, precision: 0.939, recall: 0.944, f1: 0.941
val. acc.: 0.872, loss: 0.505, precision: 0.891, recall: 0.873, f1: 0.877
train acc.: 0.949, loss: 0.152, precision: 0.948, recall: 0.952, f1: 0.950
val. acc.: 0.889, loss: 0.377, precision: 0.869, recall: 0.897, f1: 0.874
train acc.: 0.948, loss: 0.155, precision: 0.946, recall: 0.951, f1: 0.948
val. acc.: 0.881, loss: 0.456, precision: 0.903, recall: 0.882, f1: 0.889
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.903, loss: 0.338, precision: 0.878, recall: 0.911, f1: 0.882
Epoch 18/50 ━━━━━━━━━━                     36% 0:10:34 0:06:11 | best acc: 0.893wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇▇▇▇█████████
wandb:          acc/val ▁▅▇▆███████████████
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇▇▇▇█████████
wandb:           f1/val ▁▆▇▇█████▇█████████
wandb:        loss/test ▁
wandb:       loss/train █▇▅▄▅▃▃▄▂▂▂▃▂▂▂▂▂▂▃▁▁▂▂▂▁▂▁▂▂▂▂▃▂▁▂▁▁▁▂▁
wandb: loss/train_epoch █▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb:      max_acc/val ▁▅▇▇███████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▆▆▇▇▇▇▇▇████████
wandb:    precision/val ▁▄▆▆▆▇█▇▇▆▇█▇▇███▇█
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇▇▇▇▇█████████
wandb:       recall/val ▁▅▇▆███▇███████████
wandb:     scheduler_lr █████▇▇▇▆▆▆▅▅▄▄▃▂▂▁
wandb:        time/data ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        time/iter ▂▂▃▂▂▂▃▁▂▂▂▃▂▂▂▄▂▃▂▂▂▃▂▂▂▂▂▂▃▂▄▃▂▂▂▂▂█▂▂
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.9033
wandb:        acc/train 0.94846
wandb:          acc/val 0.88112
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.88183
wandb:         f1/train 0.94807
wandb:           f1/val 0.88917
wandb:        loss/test 0.33791
wandb:       loss/train 0.13365
wandb: loss/train_epoch 0.15523
wandb:         loss/val 0.45619
wandb:      max_acc/val 0.89297
wandb:   precision/test 0.87806
wandb:  precision/train 0.94574
wandb:    precision/val 0.90335
wandb:      recall/test 0.91079
wandb:     recall/train 0.95087
wandb:       recall/val 0.88225
wandb:     scheduler_lr 0.00713
wandb:        time/data 0.04136
wandb:        time/iter 0.08121
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/42q1gjbe
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_150921-42q1gjbe/logs
wandb: Agent Starting Run: 49a3eyck with config:
wandb: 	sweep: {'training': {'lr': 0.01, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_151540-49a3eyck
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/49a3eyck
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.01                                                   |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.564, loss: 1.279, precision: 0.592, recall: 0.560, f1: 0.569
new best found with: 0.645, save to logs/11_classes_full_dataset_sweep_M5/run_15/checkpoint
val. acc.: 0.645, loss: 0.982, precision: 0.734, recall: 0.656, f1: 0.667
train acc.: 0.793, loss: 0.638, precision: 0.795, recall: 0.794, f1: 0.794
new best found with: 0.798, save to logs/11_classes_full_dataset_sweep_M5/run_15/checkpoint
val. acc.: 0.798, loss: 0.634, precision: 0.782, recall: 0.813, f1: 0.782
train acc.: 0.852, loss: 0.465, precision: 0.846, recall: 0.855, f1: 0.850
new best found with: 0.812, save to logs/11_classes_full_dataset_sweep_M5/run_15/checkpoint
val. acc.: 0.812, loss: 0.544, precision: 0.851, recall: 0.819, f1: 0.822
train acc.: 0.872, loss: 0.397, precision: 0.864, recall: 0.875, f1: 0.869
new best found with: 0.844, save to logs/11_classes_full_dataset_sweep_M5/run_15/checkpoint
val. acc.: 0.844, loss: 0.473, precision: 0.846, recall: 0.851, f1: 0.842
train acc.: 0.881, loss: 0.370, precision: 0.877, recall: 0.882, f1: 0.879
new best found with: 0.873, save to logs/11_classes_full_dataset_sweep_M5/run_15/checkpoint
val. acc.: 0.873, loss: 0.410, precision: 0.859, recall: 0.881, f1: 0.863
train acc.: 0.897, loss: 0.314, precision: 0.887, recall: 0.901, f1: 0.893
val. acc.: 0.863, loss: 0.416, precision: 0.852, recall: 0.873, f1: 0.856
train acc.: 0.900, loss: 0.310, precision: 0.893, recall: 0.902, f1: 0.897
val. acc.: 0.771, loss: 0.704, precision: 0.795, recall: 0.789, f1: 0.752
train acc.: 0.911, loss: 0.273, precision: 0.903, recall: 0.914, f1: 0.907
val. acc.: 0.870, loss: 0.435, precision: 0.893, recall: 0.872, f1: 0.879
train acc.: 0.913, loss: 0.266, precision: 0.905, recall: 0.916, f1: 0.909
val. acc.: 0.860, loss: 0.441, precision: 0.892, recall: 0.861, f1: 0.871
train acc.: 0.919, loss: 0.249, precision: 0.913, recall: 0.922, f1: 0.916
val. acc.: 0.841, loss: 0.516, precision: 0.830, recall: 0.853, f1: 0.822
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.872, loss: 0.418, precision: 0.852, recall: 0.880, f1: 0.858
Epoch  9/50 ━━━━━                          18% 0:13:22 0:03:18 | best acc: 0.873wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▆▇▇▇█████
wandb:          acc/val ▁▆▆▇██▅██▇
wandb:          f1/test ▁
wandb:         f1/train ▁▆▇▇▇█████
wandb:           f1/val ▁▅▆▇▇▇▄██▆
wandb:        loss/test ▁
wandb:       loss/train ▇█▇▇▅▆▅▄▄▅▃▄▃▃▃▃▂▃▂▃▂▂▂▂▃▁▁▃▂▂▁▂▂▃▃▂▁▁▂▂
wandb: loss/train_epoch █▄▂▂▂▁▁▁▁▁
wandb:         loss/val █▄▃▂▁▁▅▁▁▂
wandb:      max_acc/val ▁▆▆▇██████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▇▇▇▇████
wandb:    precision/val ▁▃▆▆▆▆▄██▅
wandb:      recall/test ▁
wandb:     recall/train ▁▆▇▇▇█████
wandb:       recall/val ▁▆▆▇██▅█▇▇
wandb:     scheduler_lr ███▇▇▆▅▄▂▁
wandb:        time/data ▁▁▂▁▂▂▂▁█▁▁▂▁▁▁▂▁▁▂▂▄▂▂▁▂▂▁▂▂▂▁▂▂▂▁▁▂▂▂▂
wandb:        time/iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁█▁▁▁▂▁▁▂▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.87222
wandb:        acc/train 0.9194
wandb:          acc/val 0.84098
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.85782
wandb:         f1/train 0.91646
wandb:           f1/val 0.82179
wandb:        loss/test 0.41766
wandb:       loss/train 0.15047
wandb: loss/train_epoch 0.2488
wandb:         loss/val 0.51649
wandb:      max_acc/val 0.87271
wandb:   precision/test 0.85225
wandb:  precision/train 0.91257
wandb:    precision/val 0.82955
wandb:      recall/test 0.88039
wandb:     recall/train 0.92186
wandb:       recall/val 0.8532
wandb:     scheduler_lr 0.00922
wandb:        time/data 0.04839
wandb:        time/iter 0.07227
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/49a3eyck
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_151540-49a3eyck/logs
wandb: Agent Starting Run: t4iyd057 with config:
wandb: 	sweep: {'training': {'lr': 0.01, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_151908-t4iyd057
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/t4iyd057
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.01                                                   |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.566, loss: 1.272, precision: 0.595, recall: 0.561, f1: 0.571
new best found with: 0.444, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.444, loss: 1.772, precision: 0.703, recall: 0.472, f1: 0.458
train acc.: 0.801, loss: 0.628, precision: 0.805, recall: 0.802, f1: 0.803
new best found with: 0.774, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.774, loss: 0.713, precision: 0.833, recall: 0.778, f1: 0.787
train acc.: 0.851, loss: 0.460, precision: 0.846, recall: 0.853, f1: 0.849
val. acc.: 0.739, loss: 0.788, precision: 0.826, recall: 0.744, f1: 0.756
train acc.: 0.870, loss: 0.404, precision: 0.867, recall: 0.873, f1: 0.869
new best found with: 0.819, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.819, loss: 0.566, precision: 0.815, recall: 0.833, f1: 0.801
train acc.: 0.882, loss: 0.366, precision: 0.877, recall: 0.886, f1: 0.881
new best found with: 0.852, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.852, loss: 0.468, precision: 0.840, recall: 0.863, f1: 0.838
train acc.: 0.894, loss: 0.321, precision: 0.885, recall: 0.899, f1: 0.891
new best found with: 0.876, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.876, loss: 0.397, precision: 0.892, recall: 0.878, f1: 0.882
train acc.: 0.906, loss: 0.295, precision: 0.901, recall: 0.910, f1: 0.905
val. acc.: 0.824, loss: 0.563, precision: 0.825, recall: 0.837, f1: 0.799
train acc.: 0.909, loss: 0.277, precision: 0.905, recall: 0.914, f1: 0.909
val. acc.: 0.875, loss: 0.390, precision: 0.851, recall: 0.885, f1: 0.853
train acc.: 0.915, loss: 0.262, precision: 0.913, recall: 0.920, f1: 0.916
val. acc.: 0.872, loss: 0.384, precision: 0.848, recall: 0.882, f1: 0.852
train acc.: 0.920, loss: 0.247, precision: 0.916, recall: 0.922, f1: 0.919
new best found with: 0.883, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.883, loss: 0.394, precision: 0.862, recall: 0.892, f1: 0.864
train acc.: 0.918, loss: 0.251, precision: 0.915, recall: 0.923, f1: 0.918
val. acc.: 0.793, loss: 0.681, precision: 0.810, recall: 0.801, f1: 0.791
train acc.: 0.930, loss: 0.214, precision: 0.926, recall: 0.934, f1: 0.930
val. acc.: 0.872, loss: 0.442, precision: 0.898, recall: 0.873, f1: 0.880
train acc.: 0.926, loss: 0.228, precision: 0.923, recall: 0.929, f1: 0.925
val. acc.: 0.866, loss: 0.447, precision: 0.889, recall: 0.868, f1: 0.874
train acc.: 0.932, loss: 0.207, precision: 0.928, recall: 0.934, f1: 0.930
new best found with: 0.891, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.891, loss: 0.382, precision: 0.906, recall: 0.891, f1: 0.896
train acc.: 0.936, loss: 0.198, precision: 0.934, recall: 0.938, f1: 0.936
val. acc.: 0.870, loss: 0.440, precision: 0.893, recall: 0.872, f1: 0.876
train acc.: 0.938, loss: 0.188, precision: 0.937, recall: 0.941, f1: 0.939
val. acc.: 0.883, loss: 0.375, precision: 0.863, recall: 0.892, f1: 0.869
train acc.: 0.939, loss: 0.186, precision: 0.935, recall: 0.942, f1: 0.938
val. acc.: 0.875, loss: 0.432, precision: 0.857, recall: 0.884, f1: 0.854
train acc.: 0.939, loss: 0.184, precision: 0.937, recall: 0.942, f1: 0.939
new best found with: 0.894, save to logs/11_classes_full_dataset_sweep_M5/run_16/checkpoint
val. acc.: 0.894, loss: 0.361, precision: 0.914, recall: 0.897, f1: 0.902
train acc.: 0.944, loss: 0.173, precision: 0.939, recall: 0.946, f1: 0.942
val. acc.: 0.891, loss: 0.378, precision: 0.906, recall: 0.891, f1: 0.897
train acc.: 0.939, loss: 0.183, precision: 0.937, recall: 0.942, f1: 0.939
val. acc.: 0.889, loss: 0.391, precision: 0.907, recall: 0.890, f1: 0.895
train acc.: 0.945, loss: 0.169, precision: 0.942, recall: 0.947, f1: 0.944
val. acc.: 0.881, loss: 0.407, precision: 0.868, recall: 0.891, f1: 0.874
train acc.: 0.944, loss: 0.167, precision: 0.942, recall: 0.947, f1: 0.944
val. acc.: 0.888, loss: 0.385, precision: 0.904, recall: 0.891, f1: 0.895
train acc.: 0.950, loss: 0.154, precision: 0.949, recall: 0.953, f1: 0.951
val. acc.: 0.890, loss: 0.410, precision: 0.864, recall: 0.898, f1: 0.866
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.908, loss: 0.301, precision: 0.921, recall: 0.909, f1: 0.913
Epoch 22/50 ━━━━━━━━━━━━╸                  44% 0:08:39 0:07:18 | best acc: 0.894wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇▇▇▇▇████████████
wandb:          acc/val ▁▆▆▇▇█▇███▆████████████
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▆▇▇▇▇▇▇▇████████████
wandb:           f1/val ▁▆▆▆▇█▆▇▇▇▆████▇▇█████▇
wandb:        loss/test ▁
wandb:       loss/train ██▆▄▅▄▃▃▃▃▃▃▂▂▃▂▂▃▂▂▂▂▁▃▁▂▁▂▃▃▁▂▄▁▂▂▂▁▂▁
wandb: loss/train_epoch █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▃▃▂▂▁▂▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      max_acc/val ▁▆▆▇▇██████████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▆▇▇▇▇▇▇▇█▇██████████
wandb:    precision/val ▁▅▅▅▆▇▅▆▆▆▅▇▇█▇▆▆███▆█▆
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇▇▇▇▇▇████████████
wandb:       recall/val ▁▆▅▇▇█▇███▆████████████
wandb:        time/data ▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        time/iter █▂▂▄▁▂▁█▂▂▂▁▂▁▃▂▁▂▁▃▁▂▃▁▁▁▄▃▂▃▃▁▁▁▁▂▁▂▁▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.9079
wandb:        acc/train 0.95031
wandb:          acc/val 0.88953
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.91285
wandb:         f1/train 0.95098
wandb:           f1/val 0.86576
wandb:        loss/test 0.30055
wandb:       loss/train 0.16781
wandb: loss/train_epoch 0.15414
wandb:         loss/val 0.4101
wandb:      max_acc/val 0.8945
wandb:   precision/test 0.92064
wandb:  precision/train 0.94948
wandb:    precision/val 0.86427
wandb:      recall/test 0.90937
wandb:     recall/train 0.95309
wandb:       recall/val 0.89808
wandb:        time/data 0.03376
wandb:        time/iter 0.05652
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/t4iyd057
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_151908-t4iyd057/logs
wandb: Agent Starting Run: w4m966xh with config:
wandb: 	sweep: {'training': {'lr': 0.01, 'scheduler': None, 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_M5' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_152634-w4m966xh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_M5
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/n1m5gq7o
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/w4m966xh
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.01                                                   |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: M5                                           |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: waveform                                   |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_M5                                          |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_M5                                          |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_M5                 |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 25355
 - 🧊 Non-trainable: 0
 - 🤯 Total: 25355
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.564, loss: 1.279, precision: 0.592, recall: 0.560, f1: 0.569
new best found with: 0.645, save to logs/11_classes_full_dataset_sweep_M5/run_17/checkpoint
val. acc.: 0.645, loss: 0.982, precision: 0.734, recall: 0.656, f1: 0.667
train acc.: 0.791, loss: 0.647, precision: 0.791, recall: 0.792, f1: 0.791
new best found with: 0.717, save to logs/11_classes_full_dataset_sweep_M5/run_17/checkpoint
val. acc.: 0.717, loss: 0.877, precision: 0.788, recall: 0.727, f1: 0.732
train acc.: 0.847, loss: 0.468, precision: 0.842, recall: 0.848, f1: 0.844
new best found with: 0.814, save to logs/11_classes_full_dataset_sweep_M5/run_17/checkpoint
val. acc.: 0.814, loss: 0.570, precision: 0.850, recall: 0.819, f1: 0.823
train acc.: 0.878, loss: 0.387, precision: 0.873, recall: 0.882, f1: 0.877
val. acc.: 0.596, loss: 1.329, precision: 0.793, recall: 0.620, f1: 0.641
train acc.: 0.879, loss: 0.376, precision: 0.871, recall: 0.882, f1: 0.876
new best found with: 0.861, save to logs/11_classes_full_dataset_sweep_M5/run_17/checkpoint
val. acc.: 0.861, loss: 0.430, precision: 0.889, recall: 0.863, f1: 0.869
train acc.: 0.898, loss: 0.321, precision: 0.890, recall: 0.902, f1: 0.895
val. acc.: 0.853, loss: 0.483, precision: 0.875, recall: 0.855, f1: 0.861
train acc.: 0.906, loss: 0.290, precision: 0.901, recall: 0.907, f1: 0.903
val. acc.: 0.851, loss: 0.468, precision: 0.879, recall: 0.853, f1: 0.860
train acc.: 0.909, loss: 0.279, precision: 0.901, recall: 0.914, f1: 0.907
new best found with: 0.875, save to logs/11_classes_full_dataset_sweep_M5/run_17/checkpoint
val. acc.: 0.875, loss: 0.399, precision: 0.853, recall: 0.885, f1: 0.859
train acc.: 0.915, loss: 0.265, precision: 0.907, recall: 0.917, f1: 0.911
val. acc.: 0.864, loss: 0.414, precision: 0.878, recall: 0.872, f1: 0.871
train acc.: 0.917, loss: 0.255, precision: 0.909, recall: 0.919, f1: 0.913
val. acc.: 0.867, loss: 0.427, precision: 0.858, recall: 0.875, f1: 0.860
train acc.: 0.921, loss: 0.242, precision: 0.912, recall: 0.924, f1: 0.917
val. acc.: 0.865, loss: 0.419, precision: 0.843, recall: 0.875, f1: 0.841
train acc.: 0.926, loss: 0.227, precision: 0.920, recall: 0.929, f1: 0.924
val. acc.: 0.830, loss: 0.551, precision: 0.822, recall: 0.844, f1: 0.807
train acc.: 0.929, loss: 0.218, precision: 0.921, recall: 0.931, f1: 0.926
val. acc.: 0.839, loss: 0.582, precision: 0.885, recall: 0.841, f1: 0.849
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.884, loss: 0.383, precision: 0.860, recall: 0.893, f1: 0.865
Epoch 12/50 ━━━━━━╸                        24% 0:12:30 0:04:12 | best acc: 0.875wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇███████
wandb:          acc/val ▂▄▆▁█▇▇████▇▇
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇███████
wandb:           f1/val ▂▄▇▁██████▇▆▇
wandb:        loss/test ▁
wandb:       loss/train ▇▆▆▄█▅▃▄▂▄▄▃▄▂▃▃▄▄▃▃▃▁▂▂▃▂▂▃▂▂▂▂▂▂▂▂▂▁▂▃
wandb: loss/train_epoch █▄▃▂▂▂▁▁▁▁▁▁▁
wandb:         loss/val ▅▅▂█▁▂▂▁▁▁▁▂▂
wandb:      max_acc/val ▁▃▆▆█████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▇▇▇███████
wandb:    precision/val ▁▃▆▄█▇█▆▇▇▆▅█
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇███████
wandb:       recall/val ▂▄▆▁▇▇▇████▇▇
wandb:        time/data ▄▃▄▄▃▄▄▄▁▄█▄▄▃▄▄▃▄▄▄▃▄▄▄▃▄▁▃▄▄▄▄▄▃▄▇▃▃▄▄
wandb:        time/iter ▄▄▄▄▄▄▆▄▄▄▁▄█▄▁▄▅▄▄▄▄▅▄▅▄▄▄▄▄▄▆▄▅▄▄▄▄▄▄▅
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.8845
wandb:        acc/train 0.92852
wandb:          acc/val 0.83945
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.8647
wandb:         f1/train 0.9257
wandb:           f1/val 0.84947
wandb:        loss/test 0.38277
wandb:       loss/train 0.35134
wandb: loss/train_epoch 0.21794
wandb:         loss/val 0.58209
wandb:      max_acc/val 0.87538
wandb:   precision/test 0.85965
wandb:  precision/train 0.9213
wandb:    precision/val 0.88546
wandb:      recall/test 0.89327
wandb:     recall/train 0.93147
wandb:       recall/val 0.8408
wandb:        time/data 0.04182
wandb:        time/iter 0.06558
wandb: trainable_params 25355
wandb: 
wandb: 🚀 View run 11_class_M5 at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/w4m966xh
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_152634-w4m966xh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Sweep Agent: Exiting.
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 1qnemmdy with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_153109-1qnemmdy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/diikkmdn
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/1qnemmdy
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: diikkmdn
Sweep URL: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/diikkmdn
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 4                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_Conformer          |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 1217531
 - 🧊 Non-trainable: 0
 - 🤯 Total: 1217531
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.217, loss: 2.090, precision: 0.236, recall: 0.245, f1: 0.234
new best found with: 0.317, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.317, loss: 1.824, precision: 0.407, recall: 0.352, f1: 0.323
train acc.: 0.525, loss: 1.354, precision: 0.537, recall: 0.550, f1: 0.541
new best found with: 0.515, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.515, loss: 1.368, precision: 0.627, recall: 0.546, f1: 0.543
train acc.: 0.696, loss: 0.887, precision: 0.702, recall: 0.711, f1: 0.706
new best found with: 0.688, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.688, loss: 0.902, precision: 0.711, recall: 0.709, f1: 0.704
train acc.: 0.776, loss: 0.668, precision: 0.779, recall: 0.789, f1: 0.783
new best found with: 0.724, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.724, loss: 0.799, precision: 0.739, recall: 0.744, f1: 0.733
train acc.: 0.821, loss: 0.532, precision: 0.823, recall: 0.832, f1: 0.827
new best found with: 0.760, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.760, loss: 0.706, precision: 0.769, recall: 0.772, f1: 0.765
train acc.: 0.851, loss: 0.438, precision: 0.853, recall: 0.861, f1: 0.856
val. acc.: 0.742, loss: 0.796, precision: 0.761, recall: 0.759, f1: 0.750
train acc.: 0.873, loss: 0.371, precision: 0.874, recall: 0.881, f1: 0.877
new best found with: 0.768, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.768, loss: 0.759, precision: 0.793, recall: 0.768, f1: 0.777
train acc.: 0.896, loss: 0.305, precision: 0.897, recall: 0.902, f1: 0.900
new best found with: 0.782, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.782, loss: 0.740, precision: 0.793, recall: 0.796, f1: 0.793
train acc.: 0.913, loss: 0.258, precision: 0.912, recall: 0.918, f1: 0.915
val. acc.: 0.774, loss: 0.775, precision: 0.779, recall: 0.792, f1: 0.781
train acc.: 0.923, loss: 0.226, precision: 0.923, recall: 0.927, f1: 0.925
val. acc.: 0.766, loss: 0.805, precision: 0.790, recall: 0.778, f1: 0.778
train acc.: 0.935, loss: 0.193, precision: 0.937, recall: 0.938, f1: 0.937
val. acc.: 0.781, loss: 0.801, precision: 0.796, recall: 0.788, f1: 0.789
train acc.: 0.947, loss: 0.154, precision: 0.948, recall: 0.950, f1: 0.949
new best found with: 0.786, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.786, loss: 0.831, precision: 0.805, recall: 0.798, f1: 0.799
train acc.: 0.955, loss: 0.134, precision: 0.957, recall: 0.958, f1: 0.957
new best found with: 0.787, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.787, loss: 0.848, precision: 0.808, recall: 0.791, f1: 0.795
train acc.: 0.963, loss: 0.110, precision: 0.964, recall: 0.966, f1: 0.965
val. acc.: 0.753, loss: 1.043, precision: 0.785, recall: 0.756, f1: 0.762
train acc.: 0.967, loss: 0.100, precision: 0.968, recall: 0.969, f1: 0.969
val. acc.: 0.768, loss: 0.978, precision: 0.791, recall: 0.770, f1: 0.774
train acc.: 0.970, loss: 0.088, precision: 0.971, recall: 0.972, f1: 0.971
new best found with: 0.790, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.790, loss: 0.872, precision: 0.808, recall: 0.794, f1: 0.800
train acc.: 0.974, loss: 0.077, precision: 0.974, recall: 0.974, f1: 0.974
new best found with: 0.792, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.792, loss: 0.939, precision: 0.807, recall: 0.804, f1: 0.804
train acc.: 0.977, loss: 0.068, precision: 0.977, recall: 0.978, f1: 0.978
val. acc.: 0.783, loss: 0.983, precision: 0.805, recall: 0.794, f1: 0.797
train acc.: 0.980, loss: 0.061, precision: 0.981, recall: 0.981, f1: 0.981
val. acc.: 0.787, loss: 1.006, precision: 0.794, recall: 0.800, f1: 0.795
train acc.: 0.984, loss: 0.052, precision: 0.984, recall: 0.984, f1: 0.984
val. acc.: 0.773, loss: 1.062, precision: 0.799, recall: 0.786, f1: 0.787
train acc.: 0.984, loss: 0.049, precision: 0.985, recall: 0.985, f1: 0.985
val. acc.: 0.785, loss: 1.030, precision: 0.799, recall: 0.794, f1: 0.795
train acc.: 0.985, loss: 0.044, precision: 0.985, recall: 0.985, f1: 0.985
new best found with: 0.799, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.799, loss: 1.028, precision: 0.813, recall: 0.809, f1: 0.810
train acc.: 0.987, loss: 0.038, precision: 0.987, recall: 0.988, f1: 0.987
val. acc.: 0.792, loss: 1.042, precision: 0.804, recall: 0.805, f1: 0.804
train acc.: 0.991, loss: 0.030, precision: 0.991, recall: 0.991, f1: 0.991
val. acc.: 0.791, loss: 1.071, precision: 0.800, recall: 0.799, f1: 0.799
train acc.: 0.990, loss: 0.031, precision: 0.990, recall: 0.991, f1: 0.990
val. acc.: 0.794, loss: 1.053, precision: 0.809, recall: 0.800, f1: 0.803
train acc.: 0.992, loss: 0.024, precision: 0.993, recall: 0.993, f1: 0.993
val. acc.: 0.791, loss: 1.087, precision: 0.808, recall: 0.805, f1: 0.805
train acc.: 0.992, loss: 0.023, precision: 0.992, recall: 0.992, f1: 0.992
new best found with: 0.800, save to logs/11_classes_full_dataset_sweep_Conformer/run_1/checkpoint
val. acc.: 0.800, loss: 1.086, precision: 0.813, recall: 0.812, f1: 0.811
wandb: uploading wandb-summary.json; uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        acc/train ▁▄▅▆▆▇▇▇▇▇▇████████████████
wandb:          acc/val ▁▄▆▇▇▇███████▇█████████████
wandb:         f1/train ▁▄▅▆▆▇▇▇▇▇▇████████████████
wandb:           f1/val ▁▄▆▇▇▇███████▇▇████████████
wandb:       loss/train █▅▃▄▃▃▃▂▂▃▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁
wandb: loss/train_epoch █▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         loss/val █▅▂▂▁▂▁▁▁▂▂▂▂▃▃▂▂▃▃▃▃▃▃▃▃▃▃
wandb:      max_acc/val ▁▄▆▇▇▇█████████████████████
wandb:  precision/train ▁▄▅▆▆▇▇▇▇▇▇████████████████
wandb:    precision/val ▁▅▆▇▇▇██▇██████████████████
wandb:     recall/train ▁▄▅▆▆▇▇▇▇▇▇████████████████
wandb:       recall/val ▁▄▆▇▇▇▇██▇███▇▇████████████
wandb:     scheduler_lr ███████▇▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▁▁
wandb:        time/data ▁▆▅▆▆▆▆▆▇▆▆▆▆▇▇▆▇▆▇▇▆▆█▆▆▇▇▆▆▆▇▆▆▆▆▆▆▆█▆
wandb:        time/iter ▅▅▇▄▄▅▄▄▁▅█▄▅▅▅▄▄▅▅▄▅▅▅▅▅▅▆▄▄▅▅▄▄▅▅▂▅▅█▅
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:        acc/train 0.9921
wandb:          acc/val 0.80008
wandb:         base_dir logs/11_classes_full...
wandb:         f1/train 0.99222
wandb:           f1/val 0.81138
wandb:       loss/train 0.0168
wandb: loss/train_epoch 0.02322
wandb:         loss/val 1.08634
wandb:      max_acc/val 0.80008
wandb:  precision/train 0.99222
wandb:    precision/val 0.81332
wandb:     recall/train 0.99222
wandb:       recall/val 0.81164
wandb:     scheduler_lr 0.00013
wandb:        time/data 0.00162
wandb:        time/iter 1.52033
wandb: trainable_params 1217531
wandb: 
wandb: 🚀 View run 11_class_Conformer at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/1qnemmdy
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_153109-1qnemmdy/logs
wandb: ERROR Run 1qnemmdy errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
wandb: ERROR     data = self._data_queue.get(timeout=timeout)
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/queues.py", line 122, in get
wandb: ERROR     return _ForkingPickler.loads(res)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
wandb: ERROR     fd = df.detach()
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
wandb: ERROR     with _resource_sharer.get_connection(self._id) as conn:
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
wandb: ERROR     c = Client(address, authkey=process.current_process().authkey)
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/connection.py", line 502, in Client
wandb: ERROR     c = SocketClient(address)
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
wandb: ERROR     s.connect(address)
wandb: ERROR ConnectionRefusedError: [Errno 111] Connection refused
wandb: ERROR 
wandb: ERROR The above exception was the direct cause of the following exception:
wandb: ERROR 
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 27, in sweep
wandb: ERROR     engine.train()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 314, in train
wandb: ERROR     self._train_one_epoch()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 99, in _train_one_epoch
wandb: ERROR     for loader_idx, (img, label) in enumerate(self.train_loader, 1):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/accelerate/data_loader.py", line 563, in __iter__
wandb: ERROR     next_batch = next(dataloader_iter)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
wandb: ERROR     data = self._next_data()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
wandb: ERROR     idx, data = self._get_data()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1295, in _get_data
wandb: ERROR     success, data = self._try_get_data()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1146, in _try_get_data
wandb: ERROR     raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
wandb: ERROR RuntimeError: DataLoader worker (pid(s) 3304172) exited unexpectedly
wandb: ERROR 
wandb: Agent Starting Run: 7q19bgyt with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: ERROR Run 7q19bgyt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4q97ix2e with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: ERROR Run 4q97ix2e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Agent Starting Run: ce91ms7y with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.001}}
wandb: ERROR Run ce91ms7y errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: p6sgbn60 with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: ERROR Run p6sgbn60 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Agent Starting Run: ikzolhwa with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: ERROR Run ikzolhwa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: ERROR Detected 5 failed runs in a row at start, killing sweep.
wandb: To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: ofq48y7z with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_ViT' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_190927-ofq48y7z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_ViT
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/wrwimi58
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/ofq48y7z
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: wrwimi58
Sweep URL: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/wrwimi58
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: ViT                                          |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 2                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_ViT                                         |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_ViT                                         |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_ViT                |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 2756171
 - 🧊 Non-trainable: 0
 - 🤯 Total: 2756171
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     scheduler_lr ▁
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         base_dir logs/11_classes_full...
wandb:     scheduler_lr 0.0003
wandb: trainable_params 2756171
wandb: 
wandb: 🚀 View run 11_class_ViT at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/ofq48y7z
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_190927-ofq48y7z/logs
wandb: ERROR Run ofq48y7z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 27, in sweep
wandb: ERROR     engine.train()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 314, in train
wandb: ERROR     self._train_one_epoch()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 103, in _train_one_epoch
wandb: ERROR     output = self.model(img)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 853, in forward
wandb: ERROR     x = self.forward_features(x)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 827, in forward_features
wandb: ERROR     x = self.patch_embed(x)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/timm/layers/patch_embed.py", line 117, in forward
wandb: ERROR     _assert(W == self.img_size[1], f"Input width ({W}) doesn't match model ({self.img_size[1]}).")
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/__init__.py", line 1561, in _assert
wandb: ERROR     assert condition, message
wandb: ERROR AssertionError: Input width (101) doesn't match model (80).
wandb: ERROR 
wandb: Agent Starting Run: wz8fckuw with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: ERROR Run wz8fckuw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ltzqsg53 with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: ERROR Run ltzqsg53 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: ERROR Detected 3 failed runs in the first 60 seconds, killing sweep.
wandb: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: kwjax4dm with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_191006-kwjax4dm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/90g162uk
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/kwjax4dm
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: 90g162uk
Sweep URL: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/90g162uk
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 2                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 400                                                 |
| |  ├── hop_length: 160                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_Conformer          |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 1217531
 - 🧊 Non-trainable: 0
 - 🤯 Total: 1217531
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.224, loss: 2.079, precision: 0.245, recall: 0.252, f1: 0.243
new best found with: 0.357, save to logs/11_classes_full_dataset_sweep_Conformer/run_7/checkpoint
val. acc.: 0.357, loss: 1.705, precision: 0.412, recall: 0.384, f1: 0.354
train acc.: 0.556, loss: 1.273, precision: 0.565, recall: 0.578, f1: 0.569
new best found with: 0.570, save to logs/11_classes_full_dataset_sweep_Conformer/run_7/checkpoint
val. acc.: 0.570, loss: 1.194, precision: 0.642, recall: 0.598, f1: 0.583
train acc.: 0.709, loss: 0.840, precision: 0.712, recall: 0.723, f1: 0.717
new best found with: 0.700, save to logs/11_classes_full_dataset_sweep_Conformer/run_7/checkpoint
val. acc.: 0.700, loss: 0.884, precision: 0.734, recall: 0.704, f1: 0.713
train acc.: 0.778, loss: 0.649, precision: 0.779, recall: 0.790, f1: 0.784
new best found with: 0.740, save to logs/11_classes_full_dataset_sweep_Conformer/run_7/checkpoint
val. acc.: 0.740, loss: 0.754, precision: 0.749, recall: 0.757, f1: 0.749
train acc.: 0.823, loss: 0.521, precision: 0.824, recall: 0.832, f1: 0.828
new best found with: 0.755, save to logs/11_classes_full_dataset_sweep_Conformer/run_7/checkpoint
val. acc.: 0.755, loss: 0.749, precision: 0.775, recall: 0.765, f1: 0.763
train acc.: 0.850, loss: 0.441, precision: 0.850, recall: 0.858, f1: 0.854
wandb: uploading history steps 1770-1770, summary
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        acc/train ▁▅▆▇██
wandb:          acc/val ▁▅▇██
wandb:         f1/train ▁▅▆▇██
wandb:           f1/val ▁▅▇██
wandb:       loss/train ██▇█▇▇▇▇▆▇▅▅▅▅▄▃▂▃▂▂▃▂▃▂▂▂▃▁▁▁▂▂▂▂▁▂▂▂▁▁
wandb: loss/train_epoch █▅▃▂▁▁
wandb:         loss/val █▄▂▁▁
wandb:      max_acc/val ▁▅▇██
wandb:  precision/train ▁▅▆▇██
wandb:    precision/val ▁▅▇▇█
wandb:     recall/train ▁▅▆▇██
wandb:       recall/val ▁▅▇██
wandb:     scheduler_lr ██▇▅▄▁
wandb:        time/data ▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        time/iter ▂▂▁▂▂▁▂▁▁▂▂▂▂▂▂▁▂▂█▂▂▂▂▂▂▁▂▂▁▂▂▂▁▂▂▂▂▂▂▂
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:        acc/train 0.84956
wandb:          acc/val 0.75459
wandb:         base_dir logs/11_classes_full...
wandb:         f1/train 0.85379
wandb:           f1/val 0.76279
wandb:       loss/train 0.69141
wandb: loss/train_epoch 0.4409
wandb:         loss/val 0.7486
wandb:      max_acc/val 0.75459
wandb:  precision/train 0.85011
wandb:    precision/val 0.77468
wandb:     recall/train 0.85822
wandb:       recall/val 0.76522
wandb:     scheduler_lr 0.00029
wandb:        time/data 0.17499
wandb:        time/iter 0.85586
wandb: trainable_params 1217531
wandb: 
wandb: 🚀 View run 11_class_Conformer at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/kwjax4dm
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_191006-kwjax4dm/logs
wandb: ERROR Run kwjax4dm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
wandb: ERROR     data = self._data_queue.get(timeout=timeout)
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/queues.py", line 122, in get
wandb: ERROR     return _ForkingPickler.loads(res)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
wandb: ERROR     fd = df.detach()
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
wandb: ERROR     with _resource_sharer.get_connection(self._id) as conn:
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
wandb: ERROR     c = Client(address, authkey=process.current_process().authkey)
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/connection.py", line 502, in Client
wandb: ERROR     c = SocketClient(address)
wandb: ERROR   File "/home/mytkom/.local/share/uv/python/cpython-3.10.16-linux-x86_64-gnu/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
wandb: ERROR     s.connect(address)
wandb: ERROR ConnectionRefusedError: [Errno 111] Connection refused
wandb: ERROR 
wandb: ERROR The above exception was the direct cause of the following exception:
wandb: ERROR 
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 27, in sweep
wandb: ERROR     engine.train()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 317, in train
wandb: ERROR     self.validate()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 173, in validate
wandb: ERROR     for img, label in self.val_loader:
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/accelerate/data_loader.py", line 563, in __iter__
wandb: ERROR     next_batch = next(dataloader_iter)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
wandb: ERROR     data = self._next_data()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
wandb: ERROR     idx, data = self._get_data()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1295, in _get_data
wandb: ERROR     success, data = self._try_get_data()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1146, in _try_get_data
wandb: ERROR     raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e
wandb: ERROR RuntimeError: DataLoader worker (pid(s) 3366186) exited unexpectedly
wandb: ERROR 
wandb: Agent Starting Run: heetjkj3 with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: ERROR Run heetjkj3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Agent Starting Run: prfannhk with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: ERROR Run prfannhk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 91evm1ku with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.001}}
wandb: ERROR Run 91evm1ku errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Agent Starting Run: s91ahjjj with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: ERROR Run s91ahjjj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6urbh49h with config:
wandb: 	sweep: {'training': {'lr': 0.001, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: ERROR Run 6urbh49h errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 54, in <lambda>
wandb: ERROR     wandb.agent(sweep_id, function=lambda: sweep(cfg))
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/sweep.py", line 26, in sweep
wandb: ERROR     engine = build_engine(cfg.training.engine)(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/sweep_engine.py", line 10, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/train_engine.py", line 18, in __init__
wandb: ERROR     super().__init__(accelerator, cfg)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/engine/base_engine.py", line 72, in __init__
wandb: ERROR     self.live_process.start(refresh=self.live_process._renderable is not None)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/live.py", line 113, in start
wandb: ERROR     self.console.set_live(self)
wandb: ERROR   File "/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/rich/console.py", line 835, in set_live
wandb: ERROR     raise errors.LiveError("Only one live display may be active at once")
wandb: ERROR rich.errors.LiveError: Only one live display may be active at once
wandb: ERROR 
wandb: ERROR Detected 5 failed runs in a row at start, killing sweep.
wandb: To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: hkoewq85 with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_ViT' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_195809-hkoewq85
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_ViT
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/ic3dsxuv
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/hkoewq85
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: ic3dsxuv
Sweep URL: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/ic3dsxuv
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: ViT                                          |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 2                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 512                                                 |
| |  ├── hop_length: 202                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_ViT                                         |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_ViT                                         |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_ViT                |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 2756171
 - 🧊 Non-trainable: 0
 - 🤯 Total: 2756171
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
/home/mytkom/Documents/DeepLearningSpeechRecognition/.venv/lib/python3.10/site-packages/tyro/_parsers.py:332: UserWarning: The field `wandb.name` is annotated with type `<class 'str'>`, but the default value `None` has type `<class 'NoneType'>`. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(message)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: iigywa80 with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.0001}}
wandb: Currently logged in as: mytkom (dl-2-mm-jd) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250426_200311-iigywa80
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/5vuy7ikf
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/iigywa80
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
Create sweep with ID: 5vuy7ikf
Sweep URL: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/5vuy7ikf
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 2                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 512                                                 |
| |  ├── hop_length: 202                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_Conformer          |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 1217531
 - 🧊 Non-trainable: 0
 - 🤯 Total: 1217531
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.225, loss: 2.073, precision: 0.247, recall: 0.247, f1: 0.243
new best found with: 0.409, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.409, loss: 1.559, precision: 0.437, recall: 0.426, f1: 0.399
train acc.: 0.188, loss: 2.197, precision: 0.214, recall: 0.200, f1: 0.196
new best found with: 0.347, save to logs/11_classes_full_dataset_sweep_ViT/run_4/checkpoint
val. acc.: 0.347, loss: 1.796, precision: 0.479, recall: 0.354, f1: 0.351
train acc.: 0.642, loss: 1.055, precision: 0.642, recall: 0.651, f1: 0.645
new best found with: 0.715, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.715, loss: 0.832, precision: 0.735, recall: 0.731, f1: 0.721
train acc.: 0.789, loss: 0.631, precision: 0.784, recall: 0.796, f1: 0.789
new best found with: 0.782, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.782, loss: 0.647, precision: 0.781, recall: 0.795, f1: 0.782
train acc.: 0.587, loss: 1.177, precision: 0.597, recall: 0.606, f1: 0.601
new best found with: 0.718, save to logs/11_classes_full_dataset_sweep_ViT/run_4/checkpoint
val. acc.: 0.718, loss: 0.830, precision: 0.729, recall: 0.738, f1: 0.725
train acc.: 0.844, loss: 0.468, precision: 0.837, recall: 0.850, f1: 0.843
new best found with: 0.816, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.816, loss: 0.559, precision: 0.809, recall: 0.826, f1: 0.813
train acc.: 0.789, loss: 0.631, precision: 0.791, recall: 0.800, f1: 0.795
new best found with: 0.766, save to logs/11_classes_full_dataset_sweep_ViT/run_4/checkpoint
val. acc.: 0.766, loss: 0.695, precision: 0.793, recall: 0.782, f1: 0.775
train acc.: 0.876, loss: 0.371, precision: 0.870, recall: 0.881, f1: 0.874
new best found with: 0.824, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.824, loss: 0.566, precision: 0.817, recall: 0.835, f1: 0.821
train acc.: 0.899, loss: 0.303, precision: 0.893, recall: 0.904, f1: 0.898
val. acc.: 0.821, loss: 0.582, precision: 0.818, recall: 0.829, f1: 0.816
train acc.: 0.846, loss: 0.466, precision: 0.853, recall: 0.856, f1: 0.854
new best found with: 0.806, save to logs/11_classes_full_dataset_sweep_ViT/run_4/checkpoint
val. acc.: 0.806, loss: 0.566, precision: 0.831, recall: 0.820, f1: 0.817
train acc.: 0.919, loss: 0.240, precision: 0.913, recall: 0.921, f1: 0.917
new best found with: 0.826, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.826, loss: 0.625, precision: 0.843, recall: 0.816, f1: 0.826
train acc.: 0.933, loss: 0.201, precision: 0.929, recall: 0.936, f1: 0.933
new best found with: 0.836, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.836, loss: 0.593, precision: 0.827, recall: 0.845, f1: 0.832
train acc.: 0.873, loss: 0.381, precision: 0.878, recall: 0.881, f1: 0.879
new best found with: 0.840, save to logs/11_classes_full_dataset_sweep_ViT/run_4/checkpoint
val. acc.: 0.840, loss: 0.476, precision: 0.851, recall: 0.850, f1: 0.849
train acc.: 0.944, loss: 0.172, precision: 0.942, recall: 0.947, f1: 0.945
new best found with: 0.840, save to logs/11_classes_full_dataset_sweep_Conformer/run_13/checkpoint
val. acc.: 0.840, loss: 0.626, precision: 0.830, recall: 0.851, f1: 0.837
train acc.: 0.898, loss: 0.310, precision: 0.903, recall: 0.904, f1: 0.903
new best found with: 0.860, save to logs/11_classes_full_dataset_sweep_ViT/run_4/checkpoint
val. acc.: 0.860, loss: 0.453, precision: 0.872, recall: 0.869, f1: 0.868
train acc.: 0.956, loss: 0.136, precision: 0.953, recall: 0.957, f1: 0.955
val. acc.: 0.840, loss: 0.649, precision: 0.829, recall: 0.849, f1: 0.836
train acc.: 0.960, loss: 0.119, precision: 0.959, recall: 0.962, f1: 0.960
val. acc.: 0.839, loss: 0.642, precision: 0.833, recall: 0.846, f1: 0.837
train acc.: 0.907, loss: 0.266, precision: 0.913, recall: 0.913, f1: 0.913
val. acc.: 0.857, loss: 0.444, precision: 0.870, recall: 0.867, f1: 0.865
train acc.: 0.964, loss: 0.105, precision: 0.965, recall: 0.964, f1: 0.964
val. acc.: 0.830, loss: 0.718, precision: 0.836, recall: 0.833, f1: 0.831
train acc.: 0.970, loss: 0.085, precision: 0.970, recall: 0.971, f1: 0.970
val. acc.: 0.840, loss: 0.707, precision: 0.832, recall: 0.849, f1: 0.837
train acc.: 0.921, loss: 0.231, precision: 0.926, recall: 0.927, f1: 0.926
new best found with: 0.868, save to logs/11_classes_full_dataset_sweep_ViT/run_4/checkpoint
val. acc.: 0.868, loss: 0.417, precision: 0.880, recall: 0.879, f1: 0.878
train acc.: 0.976, loss: 0.074, precision: 0.976, recall: 0.976, f1: 0.976
val. acc.: 0.829, loss: 0.772, precision: 0.828, recall: 0.832, f1: 0.827
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.817, loss: 0.696, precision: 0.828, recall: 0.826, f1: 0.825
Epoch 13/50 ━━━━━━━╸                       26% -:--:-- 7:14:57 | best acc: 0.840wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇███████
wandb:          acc/val ▁▆▇███████████
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇███████
wandb:           f1/val ▁▆▇███████████
wandb:        loss/test ▁
wandb:       loss/train ███▇▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: loss/train_epoch █▄▃▂▂▂▂▁▁▁▁▁▁▁
wandb:         loss/val █▃▂▁▁▁▁▁▁▂▂▂▂▂
wandb:      max_acc/val ▁▆▇███████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▇▇▇▇███████
wandb:    precision/val ▁▆▇▇██████████
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇▇███████
wandb:       recall/val ▁▆▇███▇███████
wandb:     scheduler_lr ████▇▇▆▆▅▅▄▃▂▁
wandb:        time/data ▅▂▁▅▂▇▁▆▂▂█▆▁▁▃▅▁▂▆▄▅▆█▆▁▁▆▁▅▅▁▅▁▄▅▆▆▁▆▁
wandb:        time/iter ▁▃▅▆█▇▅▃▅▇▅▂▄▄█▅▇▆▄▆▄▃▆█▅▄▅▆▅▂▅▅█▄▃▇▅▂▂▅
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.81696
wandb:        acc/train 0.97587
wandb:          acc/val 0.82875
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.82482
wandb:         f1/train 0.97568
wandb:           f1/val 0.82676
wandb:        loss/test 0.69565
wandb:       loss/train 0.17317
wandb: loss/train_epoch 0.07447
wandb:         loss/val 0.77173
wandb:      max_acc/val 0.84021
wandb:   precision/test 0.82841
wandb:  precision/train 0.97558
wandb:    precision/val 0.82825
wandb:      recall/test 0.8259
wandb:     recall/train 0.97579
wandb:       recall/val 0.83194
wandb:     scheduler_lr 0.00025
wandb:        time/data 0.27647
wandb:        time/iter 5.36471
wandb: trainable_params 1217531
wandb: 
wandb: 🚀 View run 11_class_Conformer at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/iigywa80
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_200311-iigywa80/logs
wandb: Agent Starting Run: 8nl8yqm5 with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250427_032019-8nl8yqm5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/5vuy7ikf
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/8nl8yqm5
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 2                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 512                                                 |
| |  ├── hop_length: 202                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_Conformer          |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 1217531
 - 🧊 Non-trainable: 0
 - 🤯 Total: 1217531
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.932, loss: 0.198, precision: 0.936, recall: 0.936, f1: 0.936
train acc.: 0.225, loss: 2.073, precision: 0.247, recall: 0.247, f1: 0.243
val. acc.: 0.860, loss: 0.453, precision: 0.878, recall: 0.863, f1: 0.869
new best found with: 0.408, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.408, loss: 1.559, precision: 0.438, recall: 0.426, f1: 0.399
train acc.: 0.642, loss: 1.055, precision: 0.642, recall: 0.651, f1: 0.645
new best found with: 0.715, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.715, loss: 0.832, precision: 0.735, recall: 0.732, f1: 0.721
train acc.: 0.940, loss: 0.173, precision: 0.943, recall: 0.944, f1: 0.943
val. acc.: 0.859, loss: 0.486, precision: 0.883, recall: 0.868, f1: 0.871
train acc.: 0.789, loss: 0.631, precision: 0.784, recall: 0.795, f1: 0.788
new best found with: 0.781, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.781, loss: 0.647, precision: 0.780, recall: 0.795, f1: 0.781
train acc.: 0.844, loss: 0.469, precision: 0.837, recall: 0.850, f1: 0.842
new best found with: 0.816, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.816, loss: 0.559, precision: 0.809, recall: 0.827, f1: 0.813
train acc.: 0.951, loss: 0.146, precision: 0.954, recall: 0.954, f1: 0.954
val. acc.: 0.863, loss: 0.458, precision: 0.874, recall: 0.872, f1: 0.872
train acc.: 0.876, loss: 0.371, precision: 0.870, recall: 0.881, f1: 0.874
new best found with: 0.824, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.824, loss: 0.566, precision: 0.818, recall: 0.835, f1: 0.821
train acc.: 0.899, loss: 0.303, precision: 0.892, recall: 0.905, f1: 0.898
train acc.: 0.960, loss: 0.120, precision: 0.963, recall: 0.963, f1: 0.963
val. acc.: 0.821, loss: 0.583, precision: 0.819, recall: 0.829, f1: 0.817
val. acc.: 0.867, loss: 0.494, precision: 0.882, recall: 0.875, f1: 0.876
train acc.: 0.919, loss: 0.240, precision: 0.913, recall: 0.921, f1: 0.916
new best found with: 0.826, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.826, loss: 0.624, precision: 0.843, recall: 0.816, f1: 0.826
train acc.: 0.962, loss: 0.114, precision: 0.964, recall: 0.965, f1: 0.964
val. acc.: 0.863, loss: 0.535, precision: 0.876, recall: 0.872, f1: 0.872
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.873, loss: 0.406, precision: 0.886, recall: 0.881, f1: 0.882
Epoch 12/50 ━━━━━━╸                       24% -:--:-- 11:23:53 | best acc: 0.868wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇███████
wandb:          acc/val ▁▆▇▇█████████
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇███████
wandb:           f1/val ▁▆▇▇█████████
wandb:        loss/test ▁
wandb:       loss/train ██▇▆▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▁
wandb: loss/train_epoch █▅▃▂▂▂▂▁▁▁▁▁▁
wandb:         loss/val █▃▂▂▁▁▁▁▁▁▁▁▂
wandb:      max_acc/val ▁▆▇▇█████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▇▇▇███████
wandb:    precision/val ▁▅▆▇▇████████
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇███████
wandb:       recall/val ▁▆▇▇█████████
wandb:     scheduler_lr ████▇▇▆▆▅▄▃▂▁
wandb:        time/data ▂▂▄▂▁▁▅▃▄▁▁▃▁▃▁▂▁▁▁▂▁▁▁▂▁▄▄▁▃▁▃█▃▃▅▂▂▄▂▁
wandb:        time/iter ▅▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▃▃▄▂▃▃▃▃▃▃▃▃▃█▄▃▃▃▃▃▂
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.87337
wandb:        acc/train 0.96193
wandb:          acc/val 0.86315
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.88241
wandb:         f1/train 0.96428
wandb:           f1/val 0.87213
wandb:        loss/test 0.40581
wandb:       loss/train 0.05204
wandb: loss/train_epoch 0.11375
wandb:         loss/val 0.53456
wandb:      max_acc/val 0.86812
wandb:   precision/test 0.88622
wandb:  precision/train 0.96382
wandb:    precision/val 0.87618
wandb:      recall/test 0.88064
wandb:     recall/train 0.96475
wandb:       recall/val 0.87163
wandb:     scheduler_lr 0.00026
wandb:        time/data 0.39992
wandb:        time/iter 7.73213
wandb: trainable_params 2756171
wandb: 
wandb: 🚀 View run 11_class_ViT at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/hkoewq85
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250426_195809-hkoewq85/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: h9dvw20d with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': 'cosine', 'weight_decay': 0.001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_ViT' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250427_072509-h9dvw20d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_ViT
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/ic3dsxuv
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/h9dvw20d
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.001                                        |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: cosine                                          |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: ViT                                          |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 2                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 512                                                 |
| |  ├── hop_length: 202                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_ViT                                         |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_ViT                                         |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_ViT                |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 2756171
 - 🧊 Non-trainable: 0
 - 🤯 Total: 2756171
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 2 that is less than the current step 3. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.933, loss: 0.201, precision: 0.930, recall: 0.936, f1: 0.933
new best found with: 0.835, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.835, loss: 0.593, precision: 0.827, recall: 0.845, f1: 0.831
train acc.: 0.944, loss: 0.172, precision: 0.942, recall: 0.947, f1: 0.944
new best found with: 0.839, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.839, loss: 0.627, precision: 0.829, recall: 0.850, f1: 0.836
train acc.: 0.188, loss: 2.197, precision: 0.214, recall: 0.200, f1: 0.196
new best found with: 0.347, save to logs/11_classes_full_dataset_sweep_ViT/run_5/checkpoint
val. acc.: 0.347, loss: 1.796, precision: 0.479, recall: 0.354, f1: 0.351
train acc.: 0.956, loss: 0.136, precision: 0.953, recall: 0.957, f1: 0.955
new best found with: 0.841, save to logs/11_classes_full_dataset_sweep_Conformer/run_14/checkpoint
val. acc.: 0.841, loss: 0.648, precision: 0.830, recall: 0.850, f1: 0.837
train acc.: 0.960, loss: 0.119, precision: 0.959, recall: 0.961, f1: 0.960
val. acc.: 0.840, loss: 0.641, precision: 0.834, recall: 0.847, f1: 0.839
train acc.: 0.587, loss: 1.177, precision: 0.596, recall: 0.606, f1: 0.600
new best found with: 0.719, save to logs/11_classes_full_dataset_sweep_ViT/run_5/checkpoint
val. acc.: 0.719, loss: 0.829, precision: 0.729, recall: 0.738, f1: 0.726
train acc.: 0.963, loss: 0.105, precision: 0.964, recall: 0.964, f1: 0.964
val. acc.: 0.830, loss: 0.713, precision: 0.835, recall: 0.830, f1: 0.830
train acc.: 0.971, loss: 0.085, precision: 0.971, recall: 0.972, f1: 0.971
train acc.: 0.790, loss: 0.628, precision: 0.794, recall: 0.801, f1: 0.797
val. acc.: 0.841, loss: 0.699, precision: 0.831, recall: 0.847, f1: 0.837
new best found with: 0.762, save to logs/11_classes_full_dataset_sweep_ViT/run_5/checkpoint
val. acc.: 0.762, loss: 0.691, precision: 0.791, recall: 0.779, f1: 0.773
train acc.: 0.974, loss: 0.075, precision: 0.973, recall: 0.974, f1: 0.974
val. acc.: 0.828, loss: 0.785, precision: 0.825, recall: 0.837, f1: 0.827
train acc.: 0.847, loss: 0.467, precision: 0.853, recall: 0.857, f1: 0.855
new best found with: 0.808, save to logs/11_classes_full_dataset_sweep_ViT/run_5/checkpoint
val. acc.: 0.808, loss: 0.560, precision: 0.828, recall: 0.821, f1: 0.818
train acc.: 0.979, loss: 0.067, precision: 0.979, recall: 0.980, f1: 0.979
val. acc.: 0.821, loss: 0.792, precision: 0.821, recall: 0.819, f1: 0.817
Early stopping triggered. Stopping training.
Loading best model for testing...
Testing...
test acc.: 0.817, loss: 0.710, precision: 0.823, recall: 0.826, f1: 0.822
Epoch 14/50 ━━━━━━━━                       28% -:--:-- 7:46:03 | best acc: 0.841wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:         acc/test ▁
wandb:        acc/train ▁▅▆▇▇▇▇████████
wandb:          acc/val ▁▆▇████████████
wandb:          f1/test ▁
wandb:         f1/train ▁▅▆▇▇▇▇████████
wandb:           f1/val ▁▆▇████████████
wandb:        loss/test ▁
wandb:       loss/train ██▆▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂
wandb: loss/train_epoch █▄▃▂▂▂▂▁▁▁▁▁▁▁▁
wandb:         loss/val █▃▂▁▁▁▁▁▁▂▂▂▂▃▃
wandb:      max_acc/val ▁▆▇████████████
wandb:   precision/test ▁
wandb:  precision/train ▁▅▆▇▇▇▇████████
wandb:    precision/val ▁▆▇▇███████████
wandb:      recall/test ▁
wandb:     recall/train ▁▅▆▇▇▇▇████████
wandb:       recall/val ▁▆▇███▇███████▇
wandb:     scheduler_lr ████▇▇▇▆▆▅▄▄▃▂▁
wandb:        time/data ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:        time/iter ▅▃▆▆▄▅▆▇▇▄█▃▃▇▆▆▂▃▃▆▆█▇▅▃█▄▄▅▃▂▁▃▄▃▃▆▁▆▄
wandb: trainable_params ▁
wandb: 
wandb: Run summary:
wandb:         acc/test 0.81696
wandb:        acc/train 0.97863
wandb:          acc/val 0.82072
wandb:         base_dir logs/11_classes_full...
wandb:          f1/test 0.82238
wandb:         f1/train 0.9794
wandb:           f1/val 0.81724
wandb:        loss/test 0.70952
wandb:       loss/train 0.05668
wandb: loss/train_epoch 0.06655
wandb:         loss/val 0.7924
wandb:      max_acc/val 0.84098
wandb:   precision/test 0.82291
wandb:  precision/train 0.97929
wandb:    precision/val 0.82054
wandb:      recall/test 0.82611
wandb:     recall/train 0.97952
wandb:       recall/val 0.8187
wandb:     scheduler_lr 0.00025
wandb:        time/data 0.2562
wandb:        time/iter 5.08164
wandb: trainable_params 1217531
wandb: 
wandb: 🚀 View run 11_class_Conformer at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/8nl8yqm5
wandb: ⭐️ View project at: https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250427_032019-8nl8yqm5/logs
wandb: Agent Starting Run: jne1to9r with config:
wandb: 	sweep: {'training': {'lr': 0.0003, 'scheduler': None, 'weight_decay': 0.0001}}
wandb: WARNING Ignoring project '11_classes_full_dataset_sweep_Conformer' when running a sweep.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/mytkom/Documents/DeepLearningSpeechRecognition/wandb/run-20250427_110832-jne1to9r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 11_class_Conformer
wandb: ⭐️ View project at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels
wandb: 🧹 View sweep at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/sweeps/5vuy7ikf
wandb: 🚀 View run at https://wandb.ai/dl-2-mm-jd/Hyperparameter%20sweep%20for%20ensemble%20submodels/runs/jne1to9r
wandb: WARNING Config item 'sweep' was locked by 'sweep' (ignored update).
num_classes:  11
Class balance in training data:
  yes: 1860
  no: 1853
  up: 1843
  down: 1842
  left: 1839
  right: 1852
  on: 1864
  off: 1839
  stop: 1885
  go: 1861
  _silence_: 320
num_classes:  11
Class balance in validation data:
  down: 264
  go: 260
  left: 247
  no: 270
  off: 256
  on: 257
  right: 256
  stop: 246
  up: 260
  yes: 261
  _silence_: 39
num_classes:  11
Class balance in testing data:
  down: 253
  go: 251
  left: 267
  no: 252
  off: 262
  on: 246
  right: 259
  stop: 249
  up: 272
  yes: 256
  _silence_: 39
+-------------------------------------------------------------------+
| Configuration                                                     |
| ├── training                                                      |
| |  ├── engine: sweep_engine                                       |
| |  ├── early_stopping_patience: 5                                 |
| |  ├── label_smoothing: 0.0                                       |
| |  ├── batch_size: 64                                             |
| |  ├── val_freq: 1                                                |
| |  ├── epochs: 50                                                 |
| |  ├── num_workers: 4                                             |
| |  ├── accum_iter: 1                                              |
| |  ├── mixed_precision: no                                        |
| |  ├── lr: 0.0003                                                 |
| |  ├── weight_decay: 0.0001                                       |
| |  ├── sampling_strategy: None                                    |
| |  └── scheduler: None                                            |
| ├── model                                                         |
| |  ├── base_dim: 16                                               |
| |  ├── resume_path: None                                          |
| |  ├── architecture: Conformer                                    |
| |  └── conformer                                                  |
| |     ├── input_dim: 80                                           |
| |     ├── num_heads: 2                                            |
| |     ├── num_layers: 8                                           |
| |     ├── depthwise_conv_kernel_size: 31                          |
| |     └── dropout: 0.1                                            |
| ├── data                                                          |
| |  ├── root: data                                                 |
| |  ├── sample_rate: 16000                                         |
| |  ├── representation: mfcc                                       |
| |  ├── n_fft: 512                                                 |
| |  ├── hop_length: 202                                            |
| |  ├── n_mels: 80                                                 |
| |  ├── n_mfcc: 80                                                 |
| |  ├── yes_no_binary: False                                       |
| |  ├── unknown_commands_included: False                           |
| |  ├── silence_included: True                                     |
| |  └── unknown_binary_classification: False                       |
| ├── evaluation                                                    |
| |  ├── num_workers: 4                                             |
| |  └── batch_size: 32                                             |
| ├── wandb                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  └── tags: ['11 class']                                         |
| ├── sweep                                                         |
| |  ├── name: 11_class_Conformer                                   |
| |  ├── config: configuration/full_dataset_sweep/sweep_config.json |
| |  └── project_name: Hyperparameter sweep for ensemble submodels  |
| ├── project_tracker: ['wandb']                                    |
| ├── project_dir: 11_classes_full_dataset_sweep_Conformer          |
| ├── log_dir: logs                                                 |
| ├── mixed_precision: no                                           |
| ├── seed: 0                                                       |
| └── config: None                                                  |
+-------------------------------------------------------------------+
📁 [1mLength of dataset[0m:
 - 💪 Train: 18858
 - 📝 Validation: 2616
🤖 [1mModel Parameters:[0m
 - 🔥 Trainable: 1217531
 - 🧊 Non-trainable: 0
 - 🤯 Total: 1217531
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
wandb: WARNING Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
train acc.: 0.225, loss: 2.073, precision: 0.247, recall: 0.247, f1: 0.243
new best found with: 0.409, save to logs/11_classes_full_dataset_sweep_Conformer/run_15/checkpoint
val. acc.: 0.409, loss: 1.559, precision: 0.437, recall: 0.426, f1: 0.399
train acc.: 0.874, loss: 0.375, precision: 0.880, recall: 0.882, f1: 0.881
new best found with: 0.841, save to logs/11_classes_full_dataset_sweep_ViT/run_5/checkpoint
val. acc.: 0.841, loss: 0.475, precision: 0.852, recall: 0.852, f1: 0.850
train acc.: 0.642, loss: 1.055, precision: 0.642, recall: 0.651, f1: 0.645
new best found with: 0.715, save to logs/11_classes_full_dataset_sweep_Conformer/run_15/checkpoint
val. acc.: 0.715, loss: 0.832, precision: 0.735, recall: 0.731, f1: 0.721
train acc.: 0.789, loss: 0.631, precision: 0.784, recall: 0.795, f1: 0.789
train acc.: 0.896, loss: 0.310, precision: 0.902, recall: 0.903, f1: 0.902
new best found with: 0.782, save to logs/11_classes_full_dataset_sweep_Conformer/run_15/checkpoint
val. acc.: 0.782, loss: 0.647, precision: 0.780, recall: 0.795, f1: 0.782
new best found with: 0.860, save to logs/11_classes_full_dataset_sweep_ViT/run_5/checkpoint
val. acc.: 0.860, loss: 0.439, precision: 0.875, recall: 0.871, f1: 0.870
train acc.: 0.844, loss: 0.469, precision: 0.837, recall: 0.850, f1: 0.843
new best found with: 0.815, save to logs/11_classes_full_dataset_sweep_Conformer/run_15/checkpoint
val. acc.: 0.815, loss: 0.559, precision: 0.808, recall: 0.826, f1: 0.812
train acc.: 0.908, loss: 0.271, precision: 0.913, recall: 0.914, f1: 0.914
new best found with: 0.862, save to logs/11_classes_full_dataset_sweep_ViT/run_5/checkpoint
val. acc.: 0.862, loss: 0.438, precision: 0.869, recall: 0.871, f1: 0.868
train acc.: 0.876, loss: 0.371, precision: 0.870, recall: 0.881, f1: 0.875
new best found with: 0.822, save to logs/11_classes_full_dataset_sweep_Conformer/run_15/checkpoint
val. acc.: 0.822, loss: 0.566, precision: 0.816, recall: 0.833, f1: 0.819
train acc.: 0.899, loss: 0.304, precision: 0.892, recall: 0.904, f1: 0.898
val. acc.: 0.822, loss: 0.583, precision: 0.820, recall: 0.829, f1: 0.818
train acc.: 0.919, loss: 0.235, precision: 0.924, recall: 0.925, f1: 0.924
val. acc.: 0.861, loss: 0.423, precision: 0.872, recall: 0.870, f1: 0.868
train acc.: 0.918, loss: 0.242, precision: 0.911, recall: 0.920, f1: 0.915
new best found with: 0.822, save to logs/11_classes_full_dataset_sweep_Conformer/run_15/checkpoint
val. acc.: 0.822, loss: 0.628, precision: 0.842, recall: 0.812, f1: 0.823
Terminated
train acc.: 0.932, loss: 0.204, precision: 0.929, recall: 0.936, f1: 0.932
Terminated
